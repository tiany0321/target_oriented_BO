<!DOCTYPE html>
<html>
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">
<title></title>
<style type="text/css">
/**
 * Prism.s theme ported from highlight.js's xcode style
 */
pre code {
  padding: 1em;
}
.token.comment {
  color: #007400;
}
.token.punctuation {
  color: #999;
}
.token.tag,
.token.selector {
  color: #aa0d91;
}
.token.boolean,
.token.number,
.token.constant,
.token.symbol {
  color: #1c00cf;
}
.token.property,
.token.attr-name,
.token.string,
.token.char,
.token.builtin {
  color: #c41a16;
}
.token.inserted {
  background-color: #ccffd8;
}
.token.deleted {
  background-color: #ffebe9;
}
.token.operator,
.token.entity,
.token.url,
.language-css .token.string,
.style .token.string {
  color: #9a6e3a;
}
.token.atrule,
.token.attr-value,
.token.keyword {
  color: #836c28;
}
.token.function,
.token.class-name {
  color: #DD4A68;
}
.token.regex,
.token.important,
.token.variable {
  color: #5c2699;
}
.token.important,
.token.bold {
  font-weight: bold;
}
.token.italic {
  font-style: italic;
}
</style>
<style type="text/css">
body {
  font-family: sans-serif;
  max-width: 800px;
  margin: auto;
  padding: 1em;
  line-height: 1.5;
  box-sizing: border-box;
}
body, .footnotes, code { font-size: .9em; }
li li { font-size: .95em; }
*, *:before, *:after {
  box-sizing: inherit;
}
pre, img { max-width: 100%; }
pre, pre:hover {
  white-space: pre-wrap;
  word-break: break-all;
}
pre code {
  display: block;
  overflow-x: auto;
}
code { font-family: 'DejaVu Sans Mono', 'Droid Sans Mono', 'Lucida Console', Consolas, Monaco, monospace; }
:not(pre) > code, code[class] { background-color: #F8F8F8; }
code.language-undefined, pre > code:not([class]) {
  background-color: inherit;
  border: 1px solid #eee;
}
table {
  margin: auto;
  border-top: 1px solid #666;
}
table thead th { border-bottom: 1px solid #ddd; }
th, td { padding: 5px; }
thead, tfoot, tr:nth-child(even) { background: #eee; }
blockquote {
  color: #666;
  margin: 0;
  padding-left: 1em;
  border-left: 0.5em solid #eee;
}
hr, .footnotes::before { border: 1px dashed #ddd; }
.frontmatter { text-align: center; }
#TOC .numbered li { list-style: none; }
#TOC .numbered { padding-left: 0; }
#TOC .numbered ul { padding-left: 1em; }
table, .body h2 { border-bottom: 1px solid #666; }
.body .appendix, .appendix ~ h2 { border-bottom-style: dashed; }
.footnote-ref a::before { content: "["; }
.footnote-ref a::after { content: "]"; }
section.footnotes::before {
  content: "";
  display: block;
  max-width: 20em;
}

@media print {
  body {
    font-size: 12pt;
    max-width: 100%;
  }
  tr, img { page-break-inside: avoid; }
}
@media only screen and (min-width: 992px) {
  pre { white-space: pre; }
}
</style>
</head>
<body>
<div class="frontmatter">
<div class="title"><h1></h1></div>
<div class="author"><h2></h2></div>
<div class="date"><h3></h3></div>
</div>
<div class="body">
<pre><code class="language-r">setwd(&quot;C:\\Users\\12758\\Desktop\\target_oriented_BO\\1D\\specific\\EGO&quot;)

source(&quot;function_abs_y-t.R&quot;)

estimator=1########EGO

##############create data
total.data.x &lt;- as.data.frame(seq(0, 1, , 200))
colnames(total.data.x)=&quot;x&quot;
write.csv(total.data.x,&quot;total.data.x.csv&quot;)

total.data.y &lt;- as.data.frame(f11_xiong(total.data.x))
colnames(total.data.y)=&quot;es&quot;
order.data=as.data.frame(rep(1:nrow(total.data.x)))
colnames(order.data)=&quot;order.num&quot;
data.training=cbind(total.data.x,total.data.y,order.data)

############Set a target
tar=24
tar.points=data.training[which(abs(data.training[,&quot;es&quot;]-data.training[tar,&quot;es&quot;])&lt;=0.01*(max(data.training[,&quot;es&quot;])-min(data.training[,&quot;es&quot;]))),]
tar_data=data.training[tar,&quot;es&quot;]
data.training$pro.es=abs(data.training$es-tar_data)
write.csv(data.training$pro.es,&quot;total.data.y.csv&quot;)
tar_data=min(data.training$pro.es)
write.csv(tar_data,&quot;tar_data.csv&quot;)

print(paste(&quot;The targeted y:&quot;, tar_data))
</code></pre>
<pre><code>## [1] &quot;The targeted y: 0&quot;
</code></pre>
<pre><code class="language-r">#############iteration number
iter=seq(0,10)
i=7

#########sample the training data
set.seed(11)
sample.num0&lt;-vector()
for(ii in 1:1000){
  s &lt;-  sample(1:(nrow(data.training)),40,replace=F,prob=NULL) 
  sample.num0&lt;-as.data.frame(rbind(sample.num0,s))
}

size=c(round(0.02*nrow(data.training)))
sample.num&lt;-sample.num0[,1:(size)]

split=sample.num[i,]
split=unlist(split)
training.data &lt;- data.training[split,]
write.csv(training.data,&quot;0train.csv&quot;)
training.data.x &lt;- data.frame(training.data[,&quot;x&quot;]) 
colnames(training.data.x)=&quot;x&quot;
training.data.y &lt;-  data.frame(training.data[,&quot;pro.es&quot;]) 
colnames(training.data.y)=&quot;pro.es&quot;

############virtual data
vir.total.data.x=data.frame(data.training[,&quot;x&quot;]) 
colnames(vir.total.data.x)=&quot;x&quot;
data.all=data.frame(data.training) 

virtual.data &lt;- data.training[-split,]
rownames(virtual.data)&lt;-1:nrow(virtual.data)
virtual.data.x=as.data.frame(virtual.data[,&quot;x&quot;])
colnames(virtual.data.x)=&quot;x&quot;

##########km model prediction
gp = fn.gp(training.data.x, training.data.y,virtual.data.x)
</code></pre>
<pre><code>## 
## optimisation start
## ------------------
## * estimation method   : MLE 
## * optimisation method : BFGS 
## * analytical gradient : used
## * trend model : ~1
## * covariance model : 
##   - type :  matern5_2 
##   - noise variances :
## [1] 1e-05 1e-05 1e-05 1e-05
##   - parameters lower bounds :  1e-10 
##   - parameters upper bounds :  1.005025 
##   - variance bounds :  0.002404243 0.3060814 
##   - best initial criterion value(s) :  2.460662 
## 
## N = 2, M = 5 machine precision = 2.22045e-16
## At X0, 0 variables are exactly at the bounds
## At iterate     0  f=      -2.4607  |proj g|=      0.96275
## At iterate     1  f =      -2.6674  |proj g|=       0.91789
## At iterate     2  f =      -2.7745  |proj g|=      0.097813
## At iterate     3  f =      -2.7792  |proj g|=       0.28285
## At iterate     4  f =      -2.7822  |proj g|=      0.093416
## At iterate     5  f =      -2.7825  |proj g|=       0.02129
## At iterate     6  f =      -2.7825  |proj g|=     0.0025827
## At iterate     7  f =      -2.7825  |proj g|=    0.00023432
## 
## iterations 7
## function evaluations 11
## segments explored during Cauchy searches 8
## BFGS updates skipped 0
## active bounds at final generalized Cauchy point 0
## norm of the final projected gradient 0.000234318
## final function value -2.78246
## 
## F = -2.78246
## final  value -2.782457 
## converged
</code></pre>
<pre><code class="language-r">gp.p = fn.gp(training.data.x, training.data.y,vir.total.data.x)
</code></pre>
<pre><code>## 
## optimisation start
## ------------------
## * estimation method   : MLE 
## * optimisation method : BFGS 
## * analytical gradient : used
## * trend model : ~1
## * covariance model : 
##   - type :  matern5_2 
##   - noise variances :
## [1] 1e-05 1e-05 1e-05 1e-05
##   - parameters lower bounds :  1e-10 
##   - parameters upper bounds :  1.005025 
##   - variance bounds :  0.002404243 0.3060814 
##   - best initial criterion value(s) :  2.460662 
## 
## N = 2, M = 5 machine precision = 2.22045e-16
## At X0, 0 variables are exactly at the bounds
## At iterate     0  f=      -2.4607  |proj g|=      0.96275
## At iterate     1  f =      -2.6674  |proj g|=       0.91789
## At iterate     2  f =      -2.7745  |proj g|=      0.097813
## At iterate     3  f =      -2.7792  |proj g|=       0.28285
## At iterate     4  f =      -2.7822  |proj g|=      0.093416
## At iterate     5  f =      -2.7825  |proj g|=       0.02129
## At iterate     6  f =      -2.7825  |proj g|=     0.0025827
## At iterate     7  f =      -2.7825  |proj g|=    0.00023432
## 
## iterations 7
## function evaluations 11
## segments explored during Cauchy searches 8
## BFGS updates skipped 0
## active bounds at final generalized Cauchy point 0
## norm of the final projected gradient 0.000234318
## final function value -2.78246
## 
## F = -2.78246
## final  value -2.782457 
## converged
</code></pre>
<pre><code class="language-r">mean=as.data.frame(gp.p[,1])
mean=unlist(mean)
t=total.data.x
t=unlist(t)
yy0=cbind(total.data.x,as.data.frame(mean),as.data.frame(gp.p[,1]-gp.p[,2]),as.data.frame(gp.p[,1]+gp.p[,2]))
colnames(yy0)=c(&quot;x&quot;,&quot;mean&quot;,&quot;lower&quot;,&quot;upper&quot;)
write.csv(yy0,&quot;yy0.csv&quot;)
head(yy0)
</code></pre>
<pre><code>##                      x      mean     lower     upper
## gp.p[, 1]1 0.000000000 0.3238269 0.1700662 0.4775876
## gp.p[, 1]2 0.005025126 0.3238050 0.1700444 0.4775656
## gp.p[, 1]3 0.010050251 0.3237808 0.1700203 0.4775414
## gp.p[, 1]4 0.015075377 0.3237542 0.1699937 0.4775147
## gp.p[, 1]5 0.020100503 0.3237248 0.1699643 0.4774852
## gp.p[, 1]6 0.025125628 0.3236923 0.1699320 0.4774527
</code></pre>
<pre><code class="language-r">##########data frame
selector=1
data.OC.SD=matrix(,length(iter)-1,length(selector))
data.dis=matrix(,length(iter)-1,length(selector))
#EI=as.data.frame(EI)
SD=matrix(0,length(iter),1)
dis=matrix(0,length(iter),1)

########utility value
EI= fn.utility(data.training,estimator,training.data,gp.p)
EI[split,]=0
EI=na.omit(EI)
ee0=cbind(total.data.x,EI)
write.csv(ee0,&quot;ee0.csv&quot;)

SD[1,1]= fn.OC(data.training,training.data)
dis[1,1]= fn.OC(data.training,training.data)

######recommend the option
num= which(EI[,&quot;EI&quot;]==max(EI[,&quot;EI&quot;]))

u=1
pic0=rep(1:10)
pic0=as.character(pic0)
pic1=rep(&quot;csv&quot;,10)
pic=paste(pic0,pic1,sep=&quot;.&quot;)
er0=rep(1:10)
er0=as.character(er0)
er1=rep(&quot;csv&quot;,10)
er=paste(er0,er1,sep=&quot;ee.&quot;)
train0=rep(1:10)
train0=as.character(train0)
train1=rep(&quot;csv&quot;,10)
train=paste(train0,train1,sep=&quot;train.&quot;)

num.count=c(split,num)
training.data0=training.data
virtual.data0=virtual.data

####start iteration
u=1
repeat{
  ########updated training data and virtual data
  training.data0[nrow(training.data0)+1,]=virtual.data0[which(virtual.data0[,&quot;order.num&quot;]==num),]
  virtual.data0=virtual.data0[-which(virtual.data0[,&quot;order.num&quot;]==num),]
  rownames(training.data0)&lt;-1:nrow(training.data0)
  rownames(virtual.data0)&lt;-1:nrow(virtual.data0)
  
  training.data0.x &lt;- data.frame(training.data0[,&quot;x&quot;])
  colnames(training.data0.x)=&quot;x&quot;
  training.data0.y &lt;-  data.frame(training.data0[,&quot;pro.es&quot;]) 
  colnames(training.data0.y)=&quot;pro.es&quot;
  virtual.data0.x=as.data.frame(virtual.data0[,&quot;x&quot;])
  colnames(virtual.data0.x)=&quot;x&quot;
  
  ##########km model prediction
  gp0 = fn.gp(training.data0.x, training.data0.y,virtual.data0.x)
  gp0.p = fn.gp(training.data0.x, training.data0.y,vir.total.data.x)
  
  ########utility value
  EI=fn.utility(data.training,estimator,training.data0,gp0.p)
  EI=as.data.frame(EI)
  EI[num.count,]=0
  EI=na.omit(EI)
  
  ######recommend the option
  num= which(EI[,&quot;EI&quot;]==max(EI[,&quot;EI&quot;]))
  
  mean=as.data.frame(gp0.p[,1])
  mean=unlist(mean)
  t=total.data.x
  t=unlist(t)
  
  SD[u+1,]=fn.OC(data.training,training.data0)
  dis[u+1,]=fn.dis(data.training, training.data0[nrow(training.data0),&quot;es&quot;])
  
  if(max(EI)==0)
  {break}
  
  if (u==1|u==2|u==3|u==4|u==5|u==6)
  { 
    yy0=cbind(total.data.x,as.data.frame(mean),as.data.frame(gp0.p[,1]-gp0.p[,2]),as.data.frame(gp0.p[,1]+gp0.p[,2]))
    colnames(yy0)=c(&quot;x&quot;,&quot;mean&quot;,&quot;lower&quot;,&quot;upper&quot;)
    write.csv(yy0,pic[u])
    ee0=cbind(total.data.x,EI)
    write.csv(ee0,er[u])
    write.csv(training.data0,train[u])
    print(&quot;training.data:&quot;)
    print(training.data0)
  }
  num.count=c(num.count,num)
  #######stopping criteria
  if(u&gt;=length(iter)-1)
    break
  
  #######points in the the tolerance of t
  if(min(abs(training.data0[,&quot;es&quot;]-data.training[tar,&quot;es&quot;]))&lt;=0.01*(max(data.training[,&quot;es&quot;])-min(data.training[,&quot;es&quot;])))
  {
    repeat{ 
      u=u+1
      yy0=cbind(total.data.x,as.data.frame(mean),as.data.frame(gp0.p[,1]-gp0.p[,2]),as.data.frame(gp0.p[,1]+gp0.p[,2]))
      colnames(yy0)=c(&quot;x&quot;,&quot;mean&quot;,&quot;lower&quot;,&quot;upper&quot;)
      write.csv(yy0,pic[u])
      ee0=cbind(total.data.x,EI)
      write.csv(ee0,er[u])
      training.data0=rbind(training.data0,training.data0[nrow(training.data0),])
      write.csv(training.data0,train[u])
      if(u&gt;6)
        break
    }
  }
  u=u+1
  
}
</code></pre>
<pre><code>## 
## optimisation start
## ------------------
## * estimation method   : MLE 
## * optimisation method : BFGS 
## * analytical gradient : used
## * trend model : ~1
## * covariance model : 
##   - type :  matern5_2 
##   - noise variances :
## [1] 1e-05 1e-05 1e-05 1e-05 1e-05
##   - parameters lower bounds :  1e-10 
##   - parameters upper bounds :  1.20603 
##   - variance bounds :  0.002399856 0.2546926 
##   - best initial criterion value(s) :  3.450167 
## 
## N = 2, M = 5 machine precision = 2.22045e-16
## At X0, 0 variables are exactly at the bounds
## At iterate     0  f=      -3.4502  |proj g|=       1.1553
## At iterate     1  f =      -3.4804  |proj g|=        1.1508
## At iterate     2  f =      -3.4885  |proj g|=        1.1483
## At iterate     3  f =      -3.5073  |proj g|=       0.35676
## At iterate     4  f =      -3.5076  |proj g|=       0.23578
## At iterate     5  f =      -3.5076  |proj g|=      0.012927
## At iterate     6  f =      -3.5076  |proj g|=    0.00018997
## At iterate     7  f =      -3.5076  |proj g|=    2.1147e-06
## 
## iterations 7
## function evaluations 10
## segments explored during Cauchy searches 8
## BFGS updates skipped 0
## active bounds at final generalized Cauchy point 0
## norm of the final projected gradient 2.11468e-06
## final function value -3.50763
## 
## F = -3.50763
## final  value -3.507627 
## converged
## 
## optimisation start
## ------------------
## * estimation method   : MLE 
## * optimisation method : BFGS 
## * analytical gradient : used
## * trend model : ~1
## * covariance model : 
##   - type :  matern5_2 
##   - noise variances :
## [1] 1e-05 1e-05 1e-05 1e-05 1e-05
##   - parameters lower bounds :  1e-10 
##   - parameters upper bounds :  1.20603 
##   - variance bounds :  0.002399856 0.2546926 
##   - best initial criterion value(s) :  3.450167 
## 
## N = 2, M = 5 machine precision = 2.22045e-16
## At X0, 0 variables are exactly at the bounds
## At iterate     0  f=      -3.4502  |proj g|=       1.1553
## At iterate     1  f =      -3.4804  |proj g|=        1.1508
## At iterate     2  f =      -3.4885  |proj g|=        1.1483
## At iterate     3  f =      -3.5073  |proj g|=       0.35676
## At iterate     4  f =      -3.5076  |proj g|=       0.23578
## At iterate     5  f =      -3.5076  |proj g|=      0.012927
## At iterate     6  f =      -3.5076  |proj g|=    0.00018997
## At iterate     7  f =      -3.5076  |proj g|=    2.1147e-06
## 
## iterations 7
## function evaluations 10
## segments explored during Cauchy searches 8
## BFGS updates skipped 0
## active bounds at final generalized Cauchy point 0
## norm of the final projected gradient 2.11468e-06
## final function value -3.50763
## 
## F = -3.50763
## final  value -3.507627 
## converged
## [1] &quot;training.data:&quot;
##           x          es order.num    pro.es
## 1 0.8944724  0.09169931       179 0.2014803
## 2 0.5577889  0.45274473       112 0.5625258
## 3 0.9246231  0.08285132       185 0.1926323
## 4 0.4221106  0.14832564        85 0.2581067
## 5 0.3216080 -0.52562021        65 0.4158392
## 
## optimisation start
## ------------------
## * estimation method   : MLE 
## * optimisation method : BFGS 
## * analytical gradient : used
## * trend model : ~1
## * covariance model : 
##   - type :  matern5_2 
##   - noise variances :
## [1] 1e-05 1e-05 1e-05 1e-05 1e-05 1e-05
##   - parameters lower bounds :  1e-10 
##   - parameters upper bounds :  1.326633 
##   - variance bounds :  0.002141481 0.2353867 
##   - best initial criterion value(s) :  4.413949 
## 
## N = 2, M = 5 machine precision = 2.22045e-16
## At X0, 0 variables are exactly at the bounds
## At iterate     0  f=      -4.4139  |proj g|=       1.2708
## At iterate     1  f =       -4.641  |proj g|=        1.2545
## At iterate     2  f =      -4.6851  |proj g|=        1.2476
## At iterate     3  f =      -4.7213  |proj g|=        0.9217
## At iterate     4  f =      -4.7256  |proj g|=       0.21493
## At iterate     5  f =      -4.7259  |proj g|=       0.19861
## At iterate     6  f =      -4.7259  |proj g|=     0.0083581
## At iterate     7  f =      -4.7259  |proj g|=    4.4793e-05
## 
## iterations 7
## function evaluations 10
## segments explored during Cauchy searches 8
## BFGS updates skipped 0
## active bounds at final generalized Cauchy point 0
## norm of the final projected gradient 4.47926e-05
## final function value -4.7259
## 
## F = -4.7259
## final  value -4.725902 
## converged
## 
## optimisation start
## ------------------
## * estimation method   : MLE 
## * optimisation method : BFGS 
## * analytical gradient : used
## * trend model : ~1
## * covariance model : 
##   - type :  matern5_2 
##   - noise variances :
## [1] 1e-05 1e-05 1e-05 1e-05 1e-05 1e-05
##   - parameters lower bounds :  1e-10 
##   - parameters upper bounds :  1.326633 
##   - variance bounds :  0.002141481 0.2353867 
##   - best initial criterion value(s) :  4.413949 
## 
## N = 2, M = 5 machine precision = 2.22045e-16
## At X0, 0 variables are exactly at the bounds
## At iterate     0  f=      -4.4139  |proj g|=       1.2708
## At iterate     1  f =       -4.641  |proj g|=        1.2545
## At iterate     2  f =      -4.6851  |proj g|=        1.2476
## At iterate     3  f =      -4.7213  |proj g|=        0.9217
## At iterate     4  f =      -4.7256  |proj g|=       0.21493
## At iterate     5  f =      -4.7259  |proj g|=       0.19861
## At iterate     6  f =      -4.7259  |proj g|=     0.0083581
## At iterate     7  f =      -4.7259  |proj g|=    4.4793e-05
## 
## iterations 7
## function evaluations 10
## segments explored during Cauchy searches 8
## BFGS updates skipped 0
## active bounds at final generalized Cauchy point 0
## norm of the final projected gradient 4.47926e-05
## final function value -4.7259
## 
## F = -4.7259
## final  value -4.725902 
## converged
## [1] &quot;training.data:&quot;
##           x          es order.num    pro.es
## 1 0.8944724  0.09169931       179 0.2014803
## 2 0.5577889  0.45274473       112 0.5625258
## 3 0.9246231  0.08285132       185 0.1926323
## 4 0.4221106  0.14832564        85 0.2581067
## 5 0.3216080 -0.52562021        65 0.4158392
## 6 0.9849246  0.07852576       197 0.1883068
## 
## optimisation start
## ------------------
## * estimation method   : MLE 
## * optimisation method : BFGS 
## * analytical gradient : used
## * trend model : ~1
## * covariance model : 
##   - type :  matern5_2 
##   - noise variances :
## [1] 1e-05 1e-05 1e-05 1e-05 1e-05 1e-05 1e-05
##   - parameters lower bounds :  1e-10 
##   - parameters upper bounds :  1.326633 
##   - variance bounds :  0.001987782 0.2304034 
##   - best initial criterion value(s) :  5.542554 
## 
## N = 2, M = 5 machine precision = 2.22045e-16
## At X0, 0 variables are exactly at the bounds
## At iterate     0  f=      -5.5426  |proj g|=       1.2708
## At iterate     1  f =      -5.6914  |proj g|=        1.2639
## At iterate     2  f =      -5.8616  |proj g|=        1.2172
## At iterate     3  f =      -5.9573  |proj g|=        0.1142
## At iterate     4  f =      -5.9959  |proj g|=       0.21181
## At iterate     5  f =      -6.0109  |proj g|=       0.43211
## At iterate     6  f =      -6.0118  |proj g|=       0.18273
## At iterate     7  f =      -6.0118  |proj g|=      0.031804
## At iterate     8  f =      -6.0118  |proj g|=     0.0005779
## At iterate     9  f =      -6.0118  |proj g|=    4.3751e-05
## 
## iterations 9
## function evaluations 14
## segments explored during Cauchy searches 10
## BFGS updates skipped 0
## active bounds at final generalized Cauchy point 0
## norm of the final projected gradient 4.37509e-05
## final function value -6.01182
## 
## F = -6.01182
## final  value -6.011817 
## converged
## 
## optimisation start
## ------------------
## * estimation method   : MLE 
## * optimisation method : BFGS 
## * analytical gradient : used
## * trend model : ~1
## * covariance model : 
##   - type :  matern5_2 
##   - noise variances :
## [1] 1e-05 1e-05 1e-05 1e-05 1e-05 1e-05 1e-05
##   - parameters lower bounds :  1e-10 
##   - parameters upper bounds :  1.326633 
##   - variance bounds :  0.001987782 0.2304034 
##   - best initial criterion value(s) :  5.542554 
## 
## N = 2, M = 5 machine precision = 2.22045e-16
## At X0, 0 variables are exactly at the bounds
## At iterate     0  f=      -5.5426  |proj g|=       1.2708
## At iterate     1  f =      -5.6914  |proj g|=        1.2639
## At iterate     2  f =      -5.8616  |proj g|=        1.2172
## At iterate     3  f =      -5.9573  |proj g|=        0.1142
## At iterate     4  f =      -5.9959  |proj g|=       0.21181
## At iterate     5  f =      -6.0109  |proj g|=       0.43211
## At iterate     6  f =      -6.0118  |proj g|=       0.18273
## At iterate     7  f =      -6.0118  |proj g|=      0.031804
## At iterate     8  f =      -6.0118  |proj g|=     0.0005779
## At iterate     9  f =      -6.0118  |proj g|=    4.3751e-05
## 
## iterations 9
## function evaluations 14
## segments explored during Cauchy searches 10
## BFGS updates skipped 0
## active bounds at final generalized Cauchy point 0
## norm of the final projected gradient 4.37509e-05
## final function value -6.01182
## 
## F = -6.01182
## final  value -6.011817 
## converged
## [1] &quot;training.data:&quot;
##           x          es order.num    pro.es
## 1 0.8944724  0.09169931       179 0.2014803
## 2 0.5577889  0.45274473       112 0.5625258
## 3 0.9246231  0.08285132       185 0.1926323
## 4 0.4221106  0.14832564        85 0.2581067
## 5 0.3216080 -0.52562021        65 0.4158392
## 6 0.9849246  0.07852576       197 0.1883068
## 7 0.8040201  0.15038503       161 0.2601661
## 
## optimisation start
## ------------------
## * estimation method   : MLE 
## * optimisation method : BFGS 
## * analytical gradient : used
## * trend model : ~1
## * covariance model : 
##   - type :  matern5_2 
##   - noise variances :
## [1] 1e-05 1e-05 1e-05 1e-05 1e-05 1e-05 1e-05 1e-05
##   - parameters lower bounds :  1e-10 
##   - parameters upper bounds :  1.969849 
##   - variance bounds :  0.002192673 0.2538698 
##   - best initial criterion value(s) :  6.491035 
## 
## N = 2, M = 5 machine precision = 2.22045e-16
## At X0, 0 variables are exactly at the bounds
## At iterate     0  f=       -6.491  |proj g|=        1.887
## At iterate     1  f =      -6.5371  |proj g|=        1.8826
## At iterate     2  f =      -6.5648  |proj g|=        1.8766
## At iterate     3  f =      -6.6158  |proj g|=        0.2341
## At iterate     4  f =      -6.6164  |proj g|=       0.23378
## At iterate     5  f =      -6.6165  |proj g|=      0.045436
## At iterate     6  f =      -6.6165  |proj g|=    0.00079327
## At iterate     7  f =      -6.6165  |proj g|=    2.2658e-06
## 
## iterations 7
## function evaluations 10
## segments explored during Cauchy searches 8
## BFGS updates skipped 0
## active bounds at final generalized Cauchy point 0
## norm of the final projected gradient 2.2658e-06
## final function value -6.61648
## 
## F = -6.61648
## final  value -6.616481 
## converged
## 
## optimisation start
## ------------------
## * estimation method   : MLE 
## * optimisation method : BFGS 
## * analytical gradient : used
## * trend model : ~1
## * covariance model : 
##   - type :  matern5_2 
##   - noise variances :
## [1] 1e-05 1e-05 1e-05 1e-05 1e-05 1e-05 1e-05 1e-05
##   - parameters lower bounds :  1e-10 
##   - parameters upper bounds :  1.969849 
##   - variance bounds :  0.002192673 0.2538698 
##   - best initial criterion value(s) :  6.491035 
## 
## N = 2, M = 5 machine precision = 2.22045e-16
## At X0, 0 variables are exactly at the bounds
## At iterate     0  f=       -6.491  |proj g|=        1.887
## At iterate     1  f =      -6.5371  |proj g|=        1.8826
## At iterate     2  f =      -6.5648  |proj g|=        1.8766
## At iterate     3  f =      -6.6158  |proj g|=        0.2341
## At iterate     4  f =      -6.6164  |proj g|=       0.23378
## At iterate     5  f =      -6.6165  |proj g|=      0.045436
## At iterate     6  f =      -6.6165  |proj g|=    0.00079327
## At iterate     7  f =      -6.6165  |proj g|=    2.2658e-06
## 
## iterations 7
## function evaluations 10
## segments explored during Cauchy searches 8
## BFGS updates skipped 0
## active bounds at final generalized Cauchy point 0
## norm of the final projected gradient 2.2658e-06
## final function value -6.61648
## 
## F = -6.61648
## final  value -6.616481 
## converged
## [1] &quot;training.data:&quot;
##           x          es order.num    pro.es
## 1 0.8944724  0.09169931       179 0.2014803
## 2 0.5577889  0.45274473       112 0.5625258
## 3 0.9246231  0.08285132       185 0.1926323
## 4 0.4221106  0.14832564        85 0.2581067
## 5 0.3216080 -0.52562021        65 0.4158392
## 6 0.9849246  0.07852576       197 0.1883068
## 7 0.8040201  0.15038503       161 0.2601661
## 8 0.0000000 -0.60457741         1 0.4947964
## 
## optimisation start
## ------------------
## * estimation method   : MLE 
## * optimisation method : BFGS 
## * analytical gradient : used
## * trend model : ~1
## * covariance model : 
##   - type :  matern5_2 
##   - noise variances :
## [1] 1e-05 1e-05 1e-05 1e-05 1e-05 1e-05 1e-05 1e-05 1e-05
##   - parameters lower bounds :  1e-10 
##   - parameters upper bounds :  1.969849 
##   - variance bounds :  0.002116793 0.2782559 
##   - best initial criterion value(s) :  9.419146 
## 
## N = 2, M = 5 machine precision = 2.22045e-16
## At X0, 0 variables are exactly at the bounds
## At iterate     0  f=      -9.4191  |proj g|=        1.887
## At iterate     1  f =      -9.6468  |proj g|=        1.8794
## At iterate     2  f =      -10.069  |proj g|=       0.99485
## At iterate     3  f =      -10.072  |proj g|=       0.12893
## At iterate     4  f =      -10.072  |proj g|=       0.14046
## At iterate     5  f =      -10.072  |proj g|=     0.0089765
## At iterate     6  f =      -10.072  |proj g|=     0.0001751
## 
## iterations 6
## function evaluations 9
## segments explored during Cauchy searches 7
## BFGS updates skipped 0
## active bounds at final generalized Cauchy point 0
## norm of the final projected gradient 0.000175104
## final function value -10.072
## 
## F = -10.072
## final  value -10.072049 
## converged
## 
## optimisation start
## ------------------
## * estimation method   : MLE 
## * optimisation method : BFGS 
## * analytical gradient : used
## * trend model : ~1
## * covariance model : 
##   - type :  matern5_2 
##   - noise variances :
## [1] 1e-05 1e-05 1e-05 1e-05 1e-05 1e-05 1e-05 1e-05 1e-05
##   - parameters lower bounds :  1e-10 
##   - parameters upper bounds :  1.969849 
##   - variance bounds :  0.002116793 0.2782559 
##   - best initial criterion value(s) :  9.419146 
## 
## N = 2, M = 5 machine precision = 2.22045e-16
## At X0, 0 variables are exactly at the bounds
## At iterate     0  f=      -9.4191  |proj g|=        1.887
## At iterate     1  f =      -9.6468  |proj g|=        1.8794
## At iterate     2  f =      -10.069  |proj g|=       0.99485
## At iterate     3  f =      -10.072  |proj g|=       0.12893
## At iterate     4  f =      -10.072  |proj g|=       0.14046
## At iterate     5  f =      -10.072  |proj g|=     0.0089765
## At iterate     6  f =      -10.072  |proj g|=     0.0001751
## 
## iterations 6
## function evaluations 9
## segments explored during Cauchy searches 7
## BFGS updates skipped 0
## active bounds at final generalized Cauchy point 0
## norm of the final projected gradient 0.000175104
## final function value -10.072
## 
## F = -10.072
## final  value -10.072049 
## converged
## [1] &quot;training.data:&quot;
##           x          es order.num    pro.es
## 1 0.8944724  0.09169931       179 0.2014803
## 2 0.5577889  0.45274473       112 0.5625258
## 3 0.9246231  0.08285132       185 0.1926323
## 4 0.4221106  0.14832564        85 0.2581067
## 5 0.3216080 -0.52562021        65 0.4158392
## 6 0.9849246  0.07852576       197 0.1883068
## 7 0.8040201  0.15038503       161 0.2601661
## 8 0.0000000 -0.60457741         1 0.4947964
## 9 0.9597990  0.07834828       192 0.1881293
## 
## optimisation start
## ------------------
## * estimation method   : MLE 
## * optimisation method : BFGS 
## * analytical gradient : used
## * trend model : ~1
## * covariance model : 
##   - type :  matern5_2 
##   - noise variances :
##  [1] 1e-05 1e-05 1e-05 1e-05 1e-05 1e-05 1e-05 1e-05 1e-05 1e-05
##   - parameters lower bounds :  1e-10 
##   - parameters upper bounds :  2 
##   - variance bounds :  0.002019002 0.293114 
##   - best initial criterion value(s) :  12.52583 
## 
## N = 2, M = 5 machine precision = 2.22045e-16
## At X0, 0 variables are exactly at the bounds
## At iterate     0  f=      -12.526  |proj g|=       1.9159
## At iterate     1  f =      -13.045  |proj g|=        1.9041
## At iterate     2  f =       -13.68  |proj g|=        1.8658
## At iterate     3  f =      -13.685  |proj g|=       0.79902
## At iterate     4  f =      -13.685  |proj g|=       0.12823
## At iterate     5  f =      -13.685  |proj g|=      0.006352
## At iterate     6  f =      -13.685  |proj g|=    0.00015164
## At iterate     7  f =      -13.685  |proj g|=     5.362e-07
## 
## iterations 7
## function evaluations 10
## segments explored during Cauchy searches 8
## BFGS updates skipped 0
## active bounds at final generalized Cauchy point 0
## norm of the final projected gradient 5.36202e-07
## final function value -13.6852
## 
## F = -13.6852
## final  value -13.685169 
## converged
## 
## optimisation start
## ------------------
## * estimation method   : MLE 
## * optimisation method : BFGS 
## * analytical gradient : used
## * trend model : ~1
## * covariance model : 
##   - type :  matern5_2 
##   - noise variances :
##  [1] 1e-05 1e-05 1e-05 1e-05 1e-05 1e-05 1e-05 1e-05 1e-05 1e-05
##   - parameters lower bounds :  1e-10 
##   - parameters upper bounds :  2 
##   - variance bounds :  0.002019002 0.293114 
##   - best initial criterion value(s) :  12.52583 
## 
## N = 2, M = 5 machine precision = 2.22045e-16
## At X0, 0 variables are exactly at the bounds
## At iterate     0  f=      -12.526  |proj g|=       1.9159
## At iterate     1  f =      -13.045  |proj g|=        1.9041
## At iterate     2  f =       -13.68  |proj g|=        1.8658
## At iterate     3  f =      -13.685  |proj g|=       0.79902
## At iterate     4  f =      -13.685  |proj g|=       0.12823
## At iterate     5  f =      -13.685  |proj g|=      0.006352
## At iterate     6  f =      -13.685  |proj g|=    0.00015164
## At iterate     7  f =      -13.685  |proj g|=     5.362e-07
## 
## iterations 7
## function evaluations 10
## segments explored during Cauchy searches 8
## BFGS updates skipped 0
## active bounds at final generalized Cauchy point 0
## norm of the final projected gradient 5.36202e-07
## final function value -13.6852
## 
## F = -13.6852
## final  value -13.685169 
## converged
## [1] &quot;training.data:&quot;
##            x          es order.num    pro.es
## 1  0.8944724  0.09169931       179 0.2014803
## 2  0.5577889  0.45274473       112 0.5625258
## 3  0.9246231  0.08285132       185 0.1926323
## 4  0.4221106  0.14832564        85 0.2581067
## 5  0.3216080 -0.52562021        65 0.4158392
## 6  0.9849246  0.07852576       197 0.1883068
## 7  0.8040201  0.15038503       161 0.2601661
## 8  0.0000000 -0.60457741         1 0.4947964
## 9  0.9597990  0.07834828       192 0.1881293
## 10 1.0000000  0.07983716       200 0.1896182
## 
## optimisation start
## ------------------
## * estimation method   : MLE 
## * optimisation method : BFGS 
## * analytical gradient : used
## * trend model : ~1
## * covariance model : 
##   - type :  matern5_2 
##   - noise variances :
##  [1] 1e-05 1e-05 1e-05 1e-05 1e-05 1e-05 1e-05 1e-05 1e-05 1e-05 1e-05
##   - parameters lower bounds :  1e-10 
##   - parameters upper bounds :  2 
##   - variance bounds :  0.001921548 0.2927738 
##   - best initial criterion value(s) :  16.90927 
## 
## N = 2, M = 5 machine precision = 2.22045e-16
## At X0, 0 variables are exactly at the bounds
## At iterate     0  f=      -16.909  |proj g|=       1.9159
## At iterate     1  f =      -17.566  |proj g|=        1.9024
## At iterate     2  f =      -18.193  |proj g|=        1.8691
## At iterate     3  f =      -18.215  |proj g|=       0.26957
## At iterate     4  f =       -18.22  |proj g|=       0.26832
## At iterate     5  f =      -18.221  |proj g|=        0.2679
## At iterate     6  f =      -18.221  |proj g|=      0.013205
## At iterate     7  f =      -18.221  |proj g|=    9.0751e-05
## 
## iterations 7
## function evaluations 9
## segments explored during Cauchy searches 8
## BFGS updates skipped 0
## active bounds at final generalized Cauchy point 0
## norm of the final projected gradient 9.07512e-05
## final function value -18.2208
## 
## F = -18.2208
## final  value -18.220839 
## converged
## 
## optimisation start
## ------------------
## * estimation method   : MLE 
## * optimisation method : BFGS 
## * analytical gradient : used
## * trend model : ~1
## * covariance model : 
##   - type :  matern5_2 
##   - noise variances :
##  [1] 1e-05 1e-05 1e-05 1e-05 1e-05 1e-05 1e-05 1e-05 1e-05 1e-05 1e-05
##   - parameters lower bounds :  1e-10 
##   - parameters upper bounds :  2 
##   - variance bounds :  0.001921548 0.2927738 
##   - best initial criterion value(s) :  16.90927 
## 
## N = 2, M = 5 machine precision = 2.22045e-16
## At X0, 0 variables are exactly at the bounds
## At iterate     0  f=      -16.909  |proj g|=       1.9159
## At iterate     1  f =      -17.566  |proj g|=        1.9024
## At iterate     2  f =      -18.193  |proj g|=        1.8691
## At iterate     3  f =      -18.215  |proj g|=       0.26957
## At iterate     4  f =       -18.22  |proj g|=       0.26832
## At iterate     5  f =      -18.221  |proj g|=        0.2679
## At iterate     6  f =      -18.221  |proj g|=      0.013205
## At iterate     7  f =      -18.221  |proj g|=    9.0751e-05
## 
## iterations 7
## function evaluations 9
## segments explored during Cauchy searches 8
## BFGS updates skipped 0
## active bounds at final generalized Cauchy point 0
## norm of the final projected gradient 9.07512e-05
## final function value -18.2208
## 
## F = -18.2208
## final  value -18.220839 
## converged
## 
## optimisation start
## ------------------
## * estimation method   : MLE 
## * optimisation method : BFGS 
## * analytical gradient : used
## * trend model : ~1
## * covariance model : 
##   - type :  matern5_2 
##   - noise variances :
##  [1] 1e-05 1e-05 1e-05 1e-05 1e-05 1e-05 1e-05 1e-05 1e-05 1e-05 1e-05 1e-05
##   - parameters lower bounds :  1e-10 
##   - parameters upper bounds :  2 
##   - variance bounds :  0.001825168 0.3240052 
##   - best initial criterion value(s) :  21.22442 
## 
## N = 2, M = 5 machine precision = 2.22045e-16
## At X0, 0 variables are exactly at the bounds
## At iterate     0  f=      -21.224  |proj g|=       1.9159
## At iterate     1  f =      -22.044  |proj g|=        1.9005
## At iterate     2  f =      -22.696  |proj g|=        1.8699
## At iterate     3  f =       -22.74  |proj g|=        1.8587
## At iterate     4  f =      -22.742  |proj g|=       0.33382
## At iterate     5  f =      -22.743  |proj g|=      0.051043
## At iterate     6  f =      -22.743  |proj g|=     0.0034209
## At iterate     7  f =      -22.743  |proj g|=    2.8581e-05
## 
## iterations 7
## function evaluations 10
## segments explored during Cauchy searches 8
## BFGS updates skipped 0
## active bounds at final generalized Cauchy point 0
## norm of the final projected gradient 2.8581e-05
## final function value -22.7425
## 
## F = -22.7425
## final  value -22.742533 
## converged
## 
## optimisation start
## ------------------
## * estimation method   : MLE 
## * optimisation method : BFGS 
## * analytical gradient : used
## * trend model : ~1
## * covariance model : 
##   - type :  matern5_2 
##   - noise variances :
##  [1] 1e-05 1e-05 1e-05 1e-05 1e-05 1e-05 1e-05 1e-05 1e-05 1e-05 1e-05 1e-05
##   - parameters lower bounds :  1e-10 
##   - parameters upper bounds :  2 
##   - variance bounds :  0.001825168 0.3240052 
##   - best initial criterion value(s) :  21.22442 
## 
## N = 2, M = 5 machine precision = 2.22045e-16
## At X0, 0 variables are exactly at the bounds
## At iterate     0  f=      -21.224  |proj g|=       1.9159
## At iterate     1  f =      -22.044  |proj g|=        1.9005
## At iterate     2  f =      -22.696  |proj g|=        1.8699
## At iterate     3  f =       -22.74  |proj g|=        1.8587
## At iterate     4  f =      -22.742  |proj g|=       0.33382
## At iterate     5  f =      -22.743  |proj g|=      0.051043
## At iterate     6  f =      -22.743  |proj g|=     0.0034209
## At iterate     7  f =      -22.743  |proj g|=    2.8581e-05
## 
## iterations 7
## function evaluations 10
## segments explored during Cauchy searches 8
## BFGS updates skipped 0
## active bounds at final generalized Cauchy point 0
## norm of the final projected gradient 2.8581e-05
## final function value -22.7425
## 
## F = -22.7425
## final  value -22.742533 
## converged
## 
## optimisation start
## ------------------
## * estimation method   : MLE 
## * optimisation method : BFGS 
## * analytical gradient : used
## * trend model : ~1
## * covariance model : 
##   - type :  matern5_2 
##   - noise variances :
##  [1] 1e-05 1e-05 1e-05 1e-05 1e-05 1e-05 1e-05 1e-05 1e-05 1e-05 1e-05 1e-05 1e-05
##   - parameters lower bounds :  1e-10 
##   - parameters upper bounds :  2 
##   - variance bounds :  0.001734528 0.3240278 
##   - best initial criterion value(s) :  25.81942 
## 
## N = 2, M = 5 machine precision = 2.22045e-16
## At X0, 0 variables are exactly at the bounds
## At iterate     0  f=      -25.819  |proj g|=       1.9159
## At iterate     1  f =       -26.71  |proj g|=        1.8996
## At iterate     2  f =      -27.359  |proj g|=        1.8694
## At iterate     3  f =      -27.408  |proj g|=        1.8587
## At iterate     4  f =      -27.409  |proj g|=       0.84198
## At iterate     5  f =       -27.41  |proj g|=       0.12547
## At iterate     6  f =       -27.41  |proj g|=      0.018806
## At iterate     7  f =       -27.41  |proj g|=    0.00041968
## 
## iterations 7
## function evaluations 10
## segments explored during Cauchy searches 8
## BFGS updates skipped 0
## active bounds at final generalized Cauchy point 0
## norm of the final projected gradient 0.000419682
## final function value -27.41
## 
## F = -27.41
## final  value -27.410023 
## converged
## 
## optimisation start
## ------------------
## * estimation method   : MLE 
## * optimisation method : BFGS 
## * analytical gradient : used
## * trend model : ~1
## * covariance model : 
##   - type :  matern5_2 
##   - noise variances :
##  [1] 1e-05 1e-05 1e-05 1e-05 1e-05 1e-05 1e-05 1e-05 1e-05 1e-05 1e-05 1e-05 1e-05
##   - parameters lower bounds :  1e-10 
##   - parameters upper bounds :  2 
##   - variance bounds :  0.001734528 0.3240278 
##   - best initial criterion value(s) :  25.81942 
## 
## N = 2, M = 5 machine precision = 2.22045e-16
## At X0, 0 variables are exactly at the bounds
## At iterate     0  f=      -25.819  |proj g|=       1.9159
## At iterate     1  f =       -26.71  |proj g|=        1.8996
## At iterate     2  f =      -27.359  |proj g|=        1.8694
## At iterate     3  f =      -27.408  |proj g|=        1.8587
## At iterate     4  f =      -27.409  |proj g|=       0.84198
## At iterate     5  f =       -27.41  |proj g|=       0.12547
## At iterate     6  f =       -27.41  |proj g|=      0.018806
## At iterate     7  f =       -27.41  |proj g|=    0.00041968
## 
## iterations 7
## function evaluations 10
## segments explored during Cauchy searches 8
## BFGS updates skipped 0
## active bounds at final generalized Cauchy point 0
## norm of the final projected gradient 0.000419682
## final function value -27.41
## 
## F = -27.41
## final  value -27.410023 
## converged
## 
## optimisation start
## ------------------
## * estimation method   : MLE 
## * optimisation method : BFGS 
## * analytical gradient : used
## * trend model : ~1
## * covariance model : 
##   - type :  matern5_2 
##   - noise variances :
##  [1] 1e-05 1e-05 1e-05 1e-05 1e-05 1e-05 1e-05 1e-05 1e-05 1e-05 1e-05 1e-05 1e-05
## [14] 1e-05
##   - parameters lower bounds :  1e-10 
##   - parameters upper bounds :  2 
##   - variance bounds :  0.001649506 0.3160984 
##   - best initial criterion value(s) :  30.49433 
## 
## N = 2, M = 5 machine precision = 2.22045e-16
## At X0, 0 variables are exactly at the bounds
## At iterate     0  f=      -30.494  |proj g|=       1.9159
## At iterate     1  f =      -31.382  |proj g|=        1.8995
## At iterate     2  f =      -32.037  |proj g|=        1.8683
## At iterate     3  f =      -32.078  |proj g|=       0.29282
## At iterate     4  f =      -32.088  |proj g|=       0.29107
## At iterate     5  f =      -32.089  |proj g|=       0.29048
## At iterate     6  f =      -32.089  |proj g|=      0.035255
## At iterate     7  f =      -32.089  |proj g|=    0.00048367
## At iterate     8  f =      -32.089  |proj g|=    1.3086e-06
## 
## iterations 8
## function evaluations 10
## segments explored during Cauchy searches 9
## BFGS updates skipped 0
## active bounds at final generalized Cauchy point 0
## norm of the final projected gradient 1.30859e-06
## final function value -32.0889
## 
## F = -32.0889
## final  value -32.088884 
## converged
## 
## optimisation start
## ------------------
## * estimation method   : MLE 
## * optimisation method : BFGS 
## * analytical gradient : used
## * trend model : ~1
## * covariance model : 
##   - type :  matern5_2 
##   - noise variances :
##  [1] 1e-05 1e-05 1e-05 1e-05 1e-05 1e-05 1e-05 1e-05 1e-05 1e-05 1e-05 1e-05 1e-05
## [14] 1e-05
##   - parameters lower bounds :  1e-10 
##   - parameters upper bounds :  2 
##   - variance bounds :  0.001649506 0.3160984 
##   - best initial criterion value(s) :  30.49433 
## 
## N = 2, M = 5 machine precision = 2.22045e-16
## At X0, 0 variables are exactly at the bounds
## At iterate     0  f=      -30.494  |proj g|=       1.9159
## At iterate     1  f =      -31.382  |proj g|=        1.8995
## At iterate     2  f =      -32.037  |proj g|=        1.8683
## At iterate     3  f =      -32.078  |proj g|=       0.29282
## At iterate     4  f =      -32.088  |proj g|=       0.29107
## At iterate     5  f =      -32.089  |proj g|=       0.29048
## At iterate     6  f =      -32.089  |proj g|=      0.035255
## At iterate     7  f =      -32.089  |proj g|=    0.00048367
## At iterate     8  f =      -32.089  |proj g|=    1.3086e-06
## 
## iterations 8
## function evaluations 10
## segments explored during Cauchy searches 9
## BFGS updates skipped 0
## active bounds at final generalized Cauchy point 0
## norm of the final projected gradient 1.30859e-06
## final function value -32.0889
## 
## F = -32.0889
## final  value -32.088884 
## converged
</code></pre>
<pre><code class="language-r">write.csv(dis,&quot;dis_pre.csv&quot;)
write.csv(SD,&quot;OC_pre.csv&quot;)
</code></pre>
</div>
<script src="https://cdn.jsdelivr.net/npm/prismjs@1.29.0/components/prism-core.min.js" defer></script>
<script src="https://cdn.jsdelivr.net/npm/prismjs@1.29.0/plugins/autoloader/prism-autoloader.min.js" defer></script>
</body>
</html>
