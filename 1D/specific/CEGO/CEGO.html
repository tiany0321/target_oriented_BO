<!DOCTYPE html>
<html>
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">
<title></title>
<style type="text/css">
/**
 * Prism.s theme ported from highlight.js's xcode style
 */
pre code {
  padding: 1em;
}
.token.comment {
  color: #007400;
}
.token.punctuation {
  color: #999;
}
.token.tag,
.token.selector {
  color: #aa0d91;
}
.token.boolean,
.token.number,
.token.constant,
.token.symbol {
  color: #1c00cf;
}
.token.property,
.token.attr-name,
.token.string,
.token.char,
.token.builtin {
  color: #c41a16;
}
.token.inserted {
  background-color: #ccffd8;
}
.token.deleted {
  background-color: #ffebe9;
}
.token.operator,
.token.entity,
.token.url,
.language-css .token.string,
.style .token.string {
  color: #9a6e3a;
}
.token.atrule,
.token.attr-value,
.token.keyword {
  color: #836c28;
}
.token.function,
.token.class-name {
  color: #DD4A68;
}
.token.regex,
.token.important,
.token.variable {
  color: #5c2699;
}
.token.important,
.token.bold {
  font-weight: bold;
}
.token.italic {
  font-style: italic;
}
</style>
<style type="text/css">
body {
  font-family: sans-serif;
  max-width: 800px;
  margin: auto;
  padding: 1em;
  line-height: 1.5;
  box-sizing: border-box;
}
body, .footnotes, code { font-size: .9em; }
li li { font-size: .95em; }
*, *:before, *:after {
  box-sizing: inherit;
}
pre, img { max-width: 100%; }
pre, pre:hover {
  white-space: pre-wrap;
  word-break: break-all;
}
pre code {
  display: block;
  overflow-x: auto;
}
code { font-family: 'DejaVu Sans Mono', 'Droid Sans Mono', 'Lucida Console', Consolas, Monaco, monospace; }
:not(pre) > code, code[class] { background-color: #F8F8F8; }
code.language-undefined, pre > code:not([class]) {
  background-color: inherit;
  border: 1px solid #eee;
}
table {
  margin: auto;
  border-top: 1px solid #666;
}
table thead th { border-bottom: 1px solid #ddd; }
th, td { padding: 5px; }
thead, tfoot, tr:nth-child(even) { background: #eee; }
blockquote {
  color: #666;
  margin: 0;
  padding-left: 1em;
  border-left: 0.5em solid #eee;
}
hr, .footnotes::before { border: 1px dashed #ddd; }
.frontmatter { text-align: center; }
#TOC .numbered li { list-style: none; }
#TOC .numbered { padding-left: 0; }
#TOC .numbered ul { padding-left: 1em; }
table, .body h2 { border-bottom: 1px solid #666; }
.body .appendix, .appendix ~ h2 { border-bottom-style: dashed; }
.footnote-ref a::before { content: "["; }
.footnote-ref a::after { content: "]"; }
section.footnotes::before {
  content: "";
  display: block;
  max-width: 20em;
}

@media print {
  body {
    font-size: 12pt;
    max-width: 100%;
  }
  tr, img { page-break-inside: avoid; }
}
@media only screen and (min-width: 992px) {
  pre { white-space: pre; }
}
</style>
</head>
<body>
<div class="frontmatter">
<div class="title"><h1></h1></div>
<div class="author"><h2></h2></div>
<div class="date"><h3></h3></div>
</div>
<div class="body">
<pre><code class="language-r">setwd(&quot;C:\\Users\\12758\\Desktop\\target_oriented_BO\\1D\\specific\\CEGO&quot;)
source(&quot;function_y.R&quot;)

estimator=1########

##############create data
total.data.x &lt;- as.data.frame(seq(0, 1, , 200))
colnames(total.data.x)=&quot;x&quot;
total.data.y &lt;- as.data.frame(f11_xiong(total.data.x))
colnames(total.data.y)=&quot;es&quot;
order.data=as.data.frame(rep(1:nrow(total.data.x)))
colnames(order.data)=&quot;order.num&quot;
data.training=cbind(total.data.x,total.data.y,order.data)

############Set a target
tar=24
tar.points=data.training[which(abs(data.training[,&quot;es&quot;]-data.training[tar,&quot;es&quot;])&lt;=0.01*(max(data.training[,&quot;es&quot;])-min(data.training[,&quot;es&quot;]))),]
tar_data=data.training[tar,&quot;es&quot;]
write.csv(tar_data,&quot;tar_data.csv&quot;)

print(paste(&quot;The targeted y:&quot;, tar_data))
</code></pre>
<pre><code>## [1] &quot;The targeted y: -0.109781026409803&quot;
</code></pre>
<pre><code class="language-r">#############iteration number
iter=seq(0,10)
i=7

#########sample the training data
set.seed(11)
sample.num0&lt;-vector()
for(ii in 1:1000){
  s &lt;-  sample(1:(nrow(data.training)),40,replace=F,prob=NULL) 
  sample.num0&lt;-as.data.frame(rbind(sample.num0,s))
}

size=c(round(0.02*nrow(data.training)))
sample.num&lt;-sample.num0[,1:(size)]

split=sample.num[i,]
split=unlist(split)
training.data &lt;- data.training[split,]
write.csv(training.data,&quot;0train.csv&quot;)
training.data.x &lt;- data.frame(training.data[,&quot;x&quot;]) 
colnames(training.data.x)=&quot;x&quot;
training.data.y &lt;-  data.frame(training.data[,&quot;es&quot;]) 
colnames(training.data.y)=&quot;es&quot;

############virtual data
vir.total.data.x=data.frame(data.training[,&quot;x&quot;]) 
colnames(vir.total.data.x)=&quot;x&quot;
data.all=data.frame(data.training) 

virtual.data &lt;- data.training[-split,]
rownames(virtual.data)&lt;-1:nrow(virtual.data)
virtual.data.x=as.data.frame(virtual.data[,&quot;x&quot;])
colnames(virtual.data.x)=&quot;x&quot;

##########km model prediction
gp = fn.gp(training.data.x, training.data.y,virtual.data.x)
</code></pre>
<pre><code>## 
## optimisation start
## ------------------
## * estimation method   : MLE 
## * optimisation method : BFGS 
## * analytical gradient : used
## * trend model : ~1
## * covariance model : 
##   - type :  matern5_2 
##   - noise variances :
## [1] 1e-05 1e-05 1e-05 1e-05
##   - parameters lower bounds :  1e-10 
##   - parameters upper bounds :  1.005025 
##   - variance bounds :  0.002404243 0.3060814 
##   - best initial criterion value(s) :  2.460662 
## 
## N = 2, M = 5 machine precision = 2.22045e-16
## At X0, 0 variables are exactly at the bounds
## At iterate     0  f=      -2.4607  |proj g|=      0.96275
## At iterate     1  f =      -2.6674  |proj g|=       0.91789
## At iterate     2  f =      -2.7745  |proj g|=      0.097813
## At iterate     3  f =      -2.7792  |proj g|=       0.28285
## At iterate     4  f =      -2.7822  |proj g|=      0.093416
## At iterate     5  f =      -2.7825  |proj g|=       0.02129
## At iterate     6  f =      -2.7825  |proj g|=     0.0025827
## At iterate     7  f =      -2.7825  |proj g|=    0.00023432
## 
## iterations 7
## function evaluations 11
## segments explored during Cauchy searches 8
## BFGS updates skipped 0
## active bounds at final generalized Cauchy point 0
## norm of the final projected gradient 0.000234318
## final function value -2.78246
## 
## F = -2.78246
## final  value -2.782457 
## converged
</code></pre>
<pre><code class="language-r">gp.p = fn.gp(training.data.x, training.data.y,vir.total.data.x)
</code></pre>
<pre><code>## 
## optimisation start
## ------------------
## * estimation method   : MLE 
## * optimisation method : BFGS 
## * analytical gradient : used
## * trend model : ~1
## * covariance model : 
##   - type :  matern5_2 
##   - noise variances :
## [1] 1e-05 1e-05 1e-05 1e-05
##   - parameters lower bounds :  1e-10 
##   - parameters upper bounds :  1.005025 
##   - variance bounds :  0.002404243 0.3060814 
##   - best initial criterion value(s) :  2.460662 
## 
## N = 2, M = 5 machine precision = 2.22045e-16
## At X0, 0 variables are exactly at the bounds
## At iterate     0  f=      -2.4607  |proj g|=      0.96275
## At iterate     1  f =      -2.6674  |proj g|=       0.91789
## At iterate     2  f =      -2.7745  |proj g|=      0.097813
## At iterate     3  f =      -2.7792  |proj g|=       0.28285
## At iterate     4  f =      -2.7822  |proj g|=      0.093416
## At iterate     5  f =      -2.7825  |proj g|=       0.02129
## At iterate     6  f =      -2.7825  |proj g|=     0.0025827
## At iterate     7  f =      -2.7825  |proj g|=    0.00023432
## 
## iterations 7
## function evaluations 11
## segments explored during Cauchy searches 8
## BFGS updates skipped 0
## active bounds at final generalized Cauchy point 0
## norm of the final projected gradient 0.000234318
## final function value -2.78246
## 
## F = -2.78246
## final  value -2.782457 
## converged
</code></pre>
<pre><code class="language-r">mean=as.data.frame(gp.p[,1])
mean=unlist(mean)
t=total.data.x
t=unlist(t)
yy0=cbind(total.data.x,as.data.frame(mean),as.data.frame(gp.p[,1]-gp.p[,2]),as.data.frame(gp.p[,1]+gp.p[,2]))
colnames(yy0)=c(&quot;x&quot;,&quot;mean&quot;,&quot;lower&quot;,&quot;upper&quot;)
write.csv(yy0,&quot;yy0.csv&quot;)
head(yy0)
</code></pre>
<pre><code>##                      x      mean      lower     upper
## gp.p[, 1]1 0.000000000 0.2140459 0.06028518 0.3678065
## gp.p[, 1]2 0.005025126 0.2140240 0.06026335 0.3677846
## gp.p[, 1]3 0.010050251 0.2139998 0.06023925 0.3677604
## gp.p[, 1]4 0.015075377 0.2139732 0.06021265 0.3677337
## gp.p[, 1]5 0.020100503 0.2139438 0.06018332 0.3677042
## gp.p[, 1]6 0.025125628 0.2139113 0.06015098 0.3676716
</code></pre>
<pre><code class="language-r">##########data frame
data.OC.SD=matrix(,length(iter)-1,length(selector))
data.dis=matrix(,length(iter)-1,length(selector))
  SD=matrix(0,length(iter),1)
  dis=matrix(0,length(iter),1)

  ########utility value
  EI= fn.utility(data.training,estimator,training.data,gp.p,tar_num = tar)
  EI[split,]=0
  EI=na.omit(EI)
  ee0=cbind(total.data.x,EI)
  write.csv(ee0,&quot;ee0.csv&quot;)
  
  EI=as.data.frame(EI)

  SD[1,1]= fn.OC(data.training,training.data)
  dis[1,1]= fn.OC(data.training,training.data)

  ######recommend the option
  num= which(EI[,&quot;EI&quot;]==max(EI[,&quot;EI&quot;]))
 
  u=1
  pic0=rep(1:10)
  pic0=as.character(pic0)
  pic1=rep(&quot;csv&quot;,10)
  pic=paste(pic0,pic1,sep=&quot;.&quot;)
  er0=rep(1:10)
  er0=as.character(er0)
  er1=rep(&quot;csv&quot;,10)
  er=paste(er0,er1,sep=&quot;ee.&quot;)
  train0=rep(1:10)
  train0=as.character(train0)
  train1=rep(&quot;csv&quot;,10)
  train=paste(train0,train1,sep=&quot;train.&quot;)

  num.count=c(split,num)
  training.data0=training.data
  virtual.data0=virtual.data
  
  ####start iteration
  u=1
  repeat{
    ########updated training data and virtual data
    training.data0[nrow(training.data0)+1,]=virtual.data0[which(virtual.data0[,&quot;order.num&quot;]==num),]
    virtual.data0=virtual.data0[-which(virtual.data0[,&quot;order.num&quot;]==num),]
    rownames(training.data0)&lt;-1:nrow(training.data0)
    rownames(virtual.data0)&lt;-1:nrow(virtual.data0)
    
    training.data0.x &lt;- data.frame(training.data0[,&quot;x&quot;])
    colnames(training.data0.x)=&quot;x&quot;
    training.data0.y &lt;-  data.frame(training.data0[,&quot;es&quot;]) 
    colnames(training.data0.y)=&quot;es&quot;
    virtual.data0.x=as.data.frame(virtual.data0[,&quot;x&quot;])
    colnames(virtual.data0.x)=&quot;x&quot;
    
    ##########km model prediction
    gp0 = fn.gp(training.data0.x, training.data0.y,virtual.data0.x)
    gp0.p = fn.gp(training.data0.x, training.data0.y,vir.total.data.x)
    
    
    ########utility value
    EI=fn.utility(data.training,estimator,training.data0,gp0.p,tar_num = tar)
    EI=as.data.frame(EI)
    EI[num.count,]=0
    EI=na.omit(EI)
    
    ######recommend the option
    num= which(EI[,&quot;EI&quot;]==max(EI[,&quot;EI&quot;]))
    selector=c(&quot;T-EGO&quot;,&quot;Pre_value&quot;,&quot;Max_sd&quot;)
    mean=as.data.frame(gp0.p[,1])
    mean=unlist(mean)
    t=total.data.x
    t=unlist(t)
    
    SD[u+1,]=fn.OC(data.training,training.data0)
    dis[u+1,]=fn.dis(data.training, training.data0[nrow(training.data0),&quot;es&quot;])
    
    if(max(EI)==0)
    {break}
    
    if (u==1|u==2|u==3|u==4|u==5|u==6)
    { 
      yy0=cbind(total.data.x,as.data.frame(mean),as.data.frame(gp0.p[,1]-gp0.p[,2]),as.data.frame(gp0.p[,1]+gp0.p[,2]))
      colnames(yy0)=c(&quot;x&quot;,&quot;mean&quot;,&quot;lower&quot;,&quot;upper&quot;)
      write.csv(yy0,pic[u])
      ee0=cbind(total.data.x,EI)
      write.csv(ee0,er[u])
      write.csv(training.data0,train[u])
    }
    num.count=c(num.count,num)
    #######stopping criteria
    if(u&gt;=length(iter)-1)
      break
  
    #######points in the the tolerance of t
    if(min(abs(training.data0[,&quot;es&quot;]-data.training[tar,&quot;es&quot;]))&lt;=0.01*(max(data.training[,&quot;es&quot;])-min(data.training[,&quot;es&quot;])))
    {
      repeat{ 
        u=u+1
        yy0=cbind(total.data.x,as.data.frame(mean),as.data.frame(gp0.p[,1]-gp0.p[,2]),as.data.frame(gp0.p[,1]+gp0.p[,2]))
        colnames(yy0)=c(&quot;x&quot;,&quot;mean&quot;,&quot;lower&quot;,&quot;upper&quot;)
        write.csv(yy0,pic[u])
        ee0=cbind(total.data.x,EI)
        write.csv(ee0,er[u])
        training.data0=rbind(training.data0,training.data0[nrow(training.data0),])
        write.csv(training.data0,train[u])
        if(u&gt;6)
          break
      }
    }
    u=u+1
    
       }
</code></pre>
<pre><code>## 
## optimisation start
## ------------------
## * estimation method   : MLE 
## * optimisation method : BFGS 
## * analytical gradient : used
## * trend model : ~1
## * covariance model : 
##   - type :  matern5_2 
##   - noise variances :
## [1] 1e-05 1e-05 1e-05 1e-05 1e-05
##   - parameters lower bounds :  1e-10 
##   - parameters upper bounds :  1.155779 
##   - variance bounds :  0.002555591 0.2879653 
##   - best initial criterion value(s) :  3.822151 
## 
## N = 2, M = 5 machine precision = 2.22045e-16
## At X0, 0 variables are exactly at the bounds
## At iterate     0  f=      -3.8222  |proj g|=      0.27371
## At iterate     1  f =      -3.9776  |proj g|=         1.045
## At iterate     2  f =      -4.0829  |proj g|=        1.0423
## At iterate     3  f =      -4.1774  |proj g|=        1.0382
## At iterate     4  f =      -4.1822  |proj g|=        1.0361
## At iterate     5  f =      -4.1971  |proj g|=        1.0307
## At iterate     6  f =      -4.2088  |proj g|=       0.26189
## At iterate     7  f =      -4.2094  |proj g|=       0.24517
## At iterate     8  f =      -4.2094  |proj g|=      0.045889
## At iterate     9  f =      -4.2094  |proj g|=     0.0048677
## At iterate    10  f =      -4.2094  |proj g|=    0.00011632
## 
## iterations 10
## function evaluations 15
## segments explored during Cauchy searches 11
## BFGS updates skipped 0
## active bounds at final generalized Cauchy point 0
## norm of the final projected gradient 0.000116323
## final function value -4.20937
## 
## F = -4.20937
## final  value -4.209371 
## converged
## 
## optimisation start
## ------------------
## * estimation method   : MLE 
## * optimisation method : BFGS 
## * analytical gradient : used
## * trend model : ~1
## * covariance model : 
##   - type :  matern5_2 
##   - noise variances :
## [1] 1e-05 1e-05 1e-05 1e-05 1e-05
##   - parameters lower bounds :  1e-10 
##   - parameters upper bounds :  1.155779 
##   - variance bounds :  0.002555591 0.2879653 
##   - best initial criterion value(s) :  3.822151 
## 
## N = 2, M = 5 machine precision = 2.22045e-16
## At X0, 0 variables are exactly at the bounds
## At iterate     0  f=      -3.8222  |proj g|=      0.27371
## At iterate     1  f =      -3.9776  |proj g|=         1.045
## At iterate     2  f =      -4.0829  |proj g|=        1.0423
## At iterate     3  f =      -4.1774  |proj g|=        1.0382
## At iterate     4  f =      -4.1822  |proj g|=        1.0361
## At iterate     5  f =      -4.1971  |proj g|=        1.0307
## At iterate     6  f =      -4.2088  |proj g|=       0.26189
## At iterate     7  f =      -4.2094  |proj g|=       0.24517
## At iterate     8  f =      -4.2094  |proj g|=      0.045889
## At iterate     9  f =      -4.2094  |proj g|=     0.0048677
## At iterate    10  f =      -4.2094  |proj g|=    0.00011632
## 
## iterations 10
## function evaluations 15
## segments explored during Cauchy searches 11
## BFGS updates skipped 0
## active bounds at final generalized Cauchy point 0
## norm of the final projected gradient 0.000116323
## final function value -4.20937
## 
## F = -4.20937
## final  value -4.209371 
## converged
## 
## optimisation start
## ------------------
## * estimation method   : MLE 
## * optimisation method : BFGS 
## * analytical gradient : used
## * trend model : ~1
## * covariance model : 
##   - type :  matern5_2 
##   - noise variances :
## [1] 1e-05 1e-05 1e-05 1e-05 1e-05 1e-05
##   - parameters lower bounds :  1e-10 
##   - parameters upper bounds :  1.386935 
##   - variance bounds :  0.0114699 1.225252 
##   - best initial criterion value(s) :  -1.12865 
## 
## N = 2, M = 5 machine precision = 2.22045e-16
## At X0, 0 variables are exactly at the bounds
## At iterate     0  f=       1.1286  |proj g|=       1.3238
## At iterate     1  f =      0.30711  |proj g|=        1.2522
## At iterate     2  f =      0.25171  |proj g|=        1.0773
## At iterate     3  f =       0.2132  |proj g|=        1.0574
## At iterate     4  f =      0.18929  |proj g|=       0.75147
## At iterate     5  f =      0.18174  |proj g|=       0.33243
## At iterate     6  f =      0.17982  |proj g|=      0.099106
## At iterate     7  f =       0.1796  |proj g|=      0.019704
## At iterate     8  f =       0.1796  |proj g|=      0.046017
## At iterate     9  f =      0.17959  |proj g|=     0.0035001
## At iterate    10  f =      0.17959  |proj g|=    1.5271e-05
## At iterate    11  f =      0.17959  |proj g|=    6.0156e-06
## 
## iterations 11
## function evaluations 14
## segments explored during Cauchy searches 12
## BFGS updates skipped 0
## active bounds at final generalized Cauchy point 0
## norm of the final projected gradient 6.01556e-06
## final function value 0.17959
## 
## F = 0.17959
## final  value 0.179590 
## converged
## 
## optimisation start
## ------------------
## * estimation method   : MLE 
## * optimisation method : BFGS 
## * analytical gradient : used
## * trend model : ~1
## * covariance model : 
##   - type :  matern5_2 
##   - noise variances :
## [1] 1e-05 1e-05 1e-05 1e-05 1e-05 1e-05
##   - parameters lower bounds :  1e-10 
##   - parameters upper bounds :  1.386935 
##   - variance bounds :  0.0114699 1.225252 
##   - best initial criterion value(s) :  -1.12865 
## 
## N = 2, M = 5 machine precision = 2.22045e-16
## At X0, 0 variables are exactly at the bounds
## At iterate     0  f=       1.1286  |proj g|=       1.3238
## At iterate     1  f =      0.30711  |proj g|=        1.2522
## At iterate     2  f =      0.25171  |proj g|=        1.0773
## At iterate     3  f =       0.2132  |proj g|=        1.0574
## At iterate     4  f =      0.18929  |proj g|=       0.75147
## At iterate     5  f =      0.18174  |proj g|=       0.33243
## At iterate     6  f =      0.17982  |proj g|=      0.099106
## At iterate     7  f =       0.1796  |proj g|=      0.019704
## At iterate     8  f =       0.1796  |proj g|=      0.046017
## At iterate     9  f =      0.17959  |proj g|=     0.0035001
## At iterate    10  f =      0.17959  |proj g|=    1.5271e-05
## At iterate    11  f =      0.17959  |proj g|=    6.0156e-06
## 
## iterations 11
## function evaluations 14
## segments explored during Cauchy searches 12
## BFGS updates skipped 0
## active bounds at final generalized Cauchy point 0
## norm of the final projected gradient 6.01556e-06
## final function value 0.17959
## 
## F = 0.17959
## final  value 0.179590 
## converged
## 
## optimisation start
## ------------------
## * estimation method   : MLE 
## * optimisation method : BFGS 
## * analytical gradient : used
## * trend model : ~1
## * covariance model : 
##   - type :  matern5_2 
##   - noise variances :
## [1] 1e-05 1e-05 1e-05 1e-05 1e-05 1e-05 1e-05
##   - parameters lower bounds :  1e-10 
##   - parameters upper bounds :  1.386935 
##   - variance bounds :  0.009342884 1.111616 
##   - best initial criterion value(s) :  -0.4983863 
## 
## N = 2, M = 5 machine precision = 2.22045e-16
## At X0, 0 variables are exactly at the bounds
## At iterate     0  f=      0.49839  |proj g|=       1.3238
## At iterate     1  f =      -1.2628  |proj g|=        1.2322
## At iterate     2  f =      -1.5737  |proj g|=         1.199
## At iterate     3  f =      -1.8823  |proj g|=       0.91013
## At iterate     4  f =      -1.9936  |proj g|=       0.86504
## At iterate     5  f =      -2.0613  |proj g|=       0.81299
## At iterate     6  f =      -2.0859  |proj g|=       0.69545
## At iterate     7  f =      -2.0942  |proj g|=       0.63068
## At iterate     8  f =      -2.0969  |proj g|=      0.043507
## At iterate     9  f =       -2.097  |proj g|=       0.04062
## At iterate    10  f =       -2.097  |proj g|=     0.0071727
## At iterate    11  f =       -2.097  |proj g|=    0.00022461
## At iterate    12  f =       -2.097  |proj g|=    4.6679e-06
## 
## iterations 12
## function evaluations 16
## segments explored during Cauchy searches 13
## BFGS updates skipped 0
## active bounds at final generalized Cauchy point 0
## norm of the final projected gradient 4.66792e-06
## final function value -2.09701
## 
## F = -2.09701
## final  value -2.097009 
## converged
## 
## optimisation start
## ------------------
## * estimation method   : MLE 
## * optimisation method : BFGS 
## * analytical gradient : used
## * trend model : ~1
## * covariance model : 
##   - type :  matern5_2 
##   - noise variances :
## [1] 1e-05 1e-05 1e-05 1e-05 1e-05 1e-05 1e-05
##   - parameters lower bounds :  1e-10 
##   - parameters upper bounds :  1.386935 
##   - variance bounds :  0.009342884 1.111616 
##   - best initial criterion value(s) :  -0.4983863 
## 
## N = 2, M = 5 machine precision = 2.22045e-16
## At X0, 0 variables are exactly at the bounds
## At iterate     0  f=      0.49839  |proj g|=       1.3238
## At iterate     1  f =      -1.2628  |proj g|=        1.2322
## At iterate     2  f =      -1.5737  |proj g|=         1.199
## At iterate     3  f =      -1.8823  |proj g|=       0.91013
## At iterate     4  f =      -1.9936  |proj g|=       0.86504
## At iterate     5  f =      -2.0613  |proj g|=       0.81299
## At iterate     6  f =      -2.0859  |proj g|=       0.69545
## At iterate     7  f =      -2.0942  |proj g|=       0.63068
## At iterate     8  f =      -2.0969  |proj g|=      0.043507
## At iterate     9  f =       -2.097  |proj g|=       0.04062
## At iterate    10  f =       -2.097  |proj g|=     0.0071727
## At iterate    11  f =       -2.097  |proj g|=    0.00022461
## At iterate    12  f =       -2.097  |proj g|=    4.6679e-06
## 
## iterations 12
## function evaluations 16
## segments explored during Cauchy searches 13
## BFGS updates skipped 0
## active bounds at final generalized Cauchy point 0
## norm of the final projected gradient 4.66792e-06
## final function value -2.09701
## 
## F = -2.09701
## final  value -2.097009 
## converged
## 
## optimisation start
## ------------------
## * estimation method   : MLE 
## * optimisation method : BFGS 
## * analytical gradient : used
## * trend model : ~1
## * covariance model : 
##   - type :  matern5_2 
##   - noise variances :
## [1] 1e-05 1e-05 1e-05 1e-05 1e-05 1e-05 1e-05 1e-05
##   - parameters lower bounds :  1e-10 
##   - parameters upper bounds :  1.386935 
##   - variance bounds :  0.007885566 0.9931798 
##   - best initial criterion value(s) :  2.588173 
## 
## N = 2, M = 5 machine precision = 2.22045e-16
## At X0, 0 variables are exactly at the bounds
## At iterate     0  f=      -2.5882  |proj g|=      0.94644
## At iterate     1  f =      -4.8931  |proj g|=        1.2586
## At iterate     2  f =      -5.3214  |proj g|=        1.2421
## At iterate     3  f =      -5.9529  |proj g|=        1.1899
## At iterate     4  f =      -6.0226  |proj g|=       0.83949
## At iterate     5  f =      -6.2296  |proj g|=       0.79806
## At iterate     6  f =      -6.3464  |proj g|=       0.76191
## At iterate     7  f =      -6.4525  |proj g|=       0.71661
## At iterate     8  f =      -6.4969  |proj g|=       0.67162
## At iterate     9  f =      -6.5067  |proj g|=        1.1044
## At iterate    10  f =      -6.5238  |proj g|=      0.058139
## At iterate    11  f =      -6.5242  |proj g|=      0.083954
## At iterate    12  f =      -6.5242  |proj g|=      0.023465
## At iterate    13  f =      -6.5242  |proj g|=      0.001276
## At iterate    14  f =      -6.5242  |proj g|=    3.3888e-05
## 
## iterations 14
## function evaluations 17
## segments explored during Cauchy searches 15
## BFGS updates skipped 0
## active bounds at final generalized Cauchy point 0
## norm of the final projected gradient 3.38877e-05
## final function value -6.52422
## 
## F = -6.52422
## final  value -6.524223 
## converged
## 
## optimisation start
## ------------------
## * estimation method   : MLE 
## * optimisation method : BFGS 
## * analytical gradient : used
## * trend model : ~1
## * covariance model : 
##   - type :  matern5_2 
##   - noise variances :
## [1] 1e-05 1e-05 1e-05 1e-05 1e-05 1e-05 1e-05 1e-05
##   - parameters lower bounds :  1e-10 
##   - parameters upper bounds :  1.386935 
##   - variance bounds :  0.007885566 0.9931798 
##   - best initial criterion value(s) :  2.588173 
## 
## N = 2, M = 5 machine precision = 2.22045e-16
## At X0, 0 variables are exactly at the bounds
## At iterate     0  f=      -2.5882  |proj g|=      0.94644
## At iterate     1  f =      -4.8931  |proj g|=        1.2586
## At iterate     2  f =      -5.3214  |proj g|=        1.2421
## At iterate     3  f =      -5.9529  |proj g|=        1.1899
## At iterate     4  f =      -6.0226  |proj g|=       0.83949
## At iterate     5  f =      -6.2296  |proj g|=       0.79806
## At iterate     6  f =      -6.3464  |proj g|=       0.76191
## At iterate     7  f =      -6.4525  |proj g|=       0.71661
## At iterate     8  f =      -6.4969  |proj g|=       0.67162
## At iterate     9  f =      -6.5067  |proj g|=        1.1044
## At iterate    10  f =      -6.5238  |proj g|=      0.058139
## At iterate    11  f =      -6.5242  |proj g|=      0.083954
## At iterate    12  f =      -6.5242  |proj g|=      0.023465
## At iterate    13  f =      -6.5242  |proj g|=      0.001276
## At iterate    14  f =      -6.5242  |proj g|=    3.3888e-05
## 
## iterations 14
## function evaluations 17
## segments explored during Cauchy searches 15
## BFGS updates skipped 0
## active bounds at final generalized Cauchy point 0
## norm of the final projected gradient 3.38877e-05
## final function value -6.52422
## 
## F = -6.52422
## final  value -6.524223 
## converged
## 
## optimisation start
## ------------------
## * estimation method   : MLE 
## * optimisation method : BFGS 
## * analytical gradient : used
## * trend model : ~1
## * covariance model : 
##   - type :  matern5_2 
##   - noise variances :
## [1] 1e-05 1e-05 1e-05 1e-05 1e-05 1e-05 1e-05 1e-05 1e-05
##   - parameters lower bounds :  1e-10 
##   - parameters upper bounds :  1.386935 
##   - variance bounds :  0.006913788 0.8854258 
##   - best initial criterion value(s) :  6.24022 
## 
## N = 2, M = 5 machine precision = 2.22045e-16
## At X0, 0 variables are exactly at the bounds
## At iterate     0  f=      -6.2402  |proj g|=       1.3238
## At iterate     1  f =      -9.4166  |proj g|=         1.222
## At iterate     2  f =      -10.033  |proj g|=        1.2056
## At iterate     3  f =      -10.451  |proj g|=       0.73038
## At iterate     4  f =      -10.691  |proj g|=       0.68797
## At iterate     5  f =      -10.808  |proj g|=       0.65051
## At iterate     6  f =      -10.903  |proj g|=       0.60723
## At iterate     7  f =      -10.925  |proj g|=        1.1225
## At iterate     8  f =      -10.991  |proj g|=         0.412
## At iterate     9  f =      -11.003  |proj g|=       0.43818
## At iterate    10  f =      -11.008  |proj g|=       0.30806
## At iterate    11  f =      -11.009  |proj g|=      0.034951
## At iterate    12  f =      -11.009  |proj g|=     0.0046584
## At iterate    13  f =      -11.009  |proj g|=      0.002914
## At iterate    14  f =      -11.009  |proj g|=    7.3994e-05
## 
## iterations 14
## function evaluations 18
## segments explored during Cauchy searches 15
## BFGS updates skipped 0
## active bounds at final generalized Cauchy point 0
## norm of the final projected gradient 7.39942e-05
## final function value -11.0093
## 
## F = -11.0093
## final  value -11.009254 
## converged
## 
## optimisation start
## ------------------
## * estimation method   : MLE 
## * optimisation method : BFGS 
## * analytical gradient : used
## * trend model : ~1
## * covariance model : 
##   - type :  matern5_2 
##   - noise variances :
## [1] 1e-05 1e-05 1e-05 1e-05 1e-05 1e-05 1e-05 1e-05 1e-05
##   - parameters lower bounds :  1e-10 
##   - parameters upper bounds :  1.386935 
##   - variance bounds :  0.006913788 0.8854258 
##   - best initial criterion value(s) :  6.24022 
## 
## N = 2, M = 5 machine precision = 2.22045e-16
## At X0, 0 variables are exactly at the bounds
## At iterate     0  f=      -6.2402  |proj g|=       1.3238
## At iterate     1  f =      -9.4166  |proj g|=         1.222
## At iterate     2  f =      -10.033  |proj g|=        1.2056
## At iterate     3  f =      -10.451  |proj g|=       0.73038
## At iterate     4  f =      -10.691  |proj g|=       0.68797
## At iterate     5  f =      -10.808  |proj g|=       0.65051
## At iterate     6  f =      -10.903  |proj g|=       0.60723
## At iterate     7  f =      -10.925  |proj g|=        1.1225
## At iterate     8  f =      -10.991  |proj g|=         0.412
## At iterate     9  f =      -11.003  |proj g|=       0.43818
## At iterate    10  f =      -11.008  |proj g|=       0.30806
## At iterate    11  f =      -11.009  |proj g|=      0.034951
## At iterate    12  f =      -11.009  |proj g|=     0.0046584
## At iterate    13  f =      -11.009  |proj g|=      0.002914
## At iterate    14  f =      -11.009  |proj g|=    7.3994e-05
## 
## iterations 14
## function evaluations 18
## segments explored during Cauchy searches 15
## BFGS updates skipped 0
## active bounds at final generalized Cauchy point 0
## norm of the final projected gradient 7.39942e-05
## final function value -11.0093
## 
## F = -11.0093
## final  value -11.009254 
## converged
## 
## optimisation start
## ------------------
## * estimation method   : MLE 
## * optimisation method : BFGS 
## * analytical gradient : used
## * trend model : ~1
## * covariance model : 
##   - type :  matern5_2 
##   - noise variances :
##  [1] 1e-05 1e-05 1e-05 1e-05 1e-05 1e-05 1e-05 1e-05 1e-05 1e-05
##   - parameters lower bounds :  1e-10 
##   - parameters upper bounds :  1.386935 
##   - variance bounds :  0.007923257 0.8474102 
##   - best initial criterion value(s) :  10.72763 
## 
## N = 2, M = 5 machine precision = 2.22045e-16
## At X0, 0 variables are exactly at the bounds
## At iterate     0  f=      -10.728  |proj g|=       0.8044
## At iterate     1  f =      -10.957  |proj g|=        1.2817
## At iterate     2  f =      -13.556  |proj g|=        1.2252
## At iterate     3  f =      -15.227  |proj g|=        1.1513
## At iterate     4  f =       -15.37  |proj g|=       0.59625
## At iterate     5  f =      -15.379  |proj g|=       0.59258
## At iterate     6  f =      -15.422  |proj g|=        1.1185
## At iterate     7  f =      -15.477  |proj g|=        1.1044
## At iterate     8  f =      -15.513  |proj g|=       0.46151
## At iterate     9  f =      -15.517  |proj g|=        0.3865
## At iterate    10  f =      -15.521  |proj g|=       0.10777
## At iterate    11  f =      -15.521  |proj g|=      0.021783
## At iterate    12  f =      -15.521  |proj g|=    0.00051954
## At iterate    13  f =      -15.521  |proj g|=    0.00011162
## 
## iterations 13
## function evaluations 15
## segments explored during Cauchy searches 14
## BFGS updates skipped 0
## active bounds at final generalized Cauchy point 0
## norm of the final projected gradient 0.000111617
## final function value -15.5211
## 
## F = -15.5211
## final  value -15.521096 
## converged
## 
## optimisation start
## ------------------
## * estimation method   : MLE 
## * optimisation method : BFGS 
## * analytical gradient : used
## * trend model : ~1
## * covariance model : 
##   - type :  matern5_2 
##   - noise variances :
##  [1] 1e-05 1e-05 1e-05 1e-05 1e-05 1e-05 1e-05 1e-05 1e-05 1e-05
##   - parameters lower bounds :  1e-10 
##   - parameters upper bounds :  1.386935 
##   - variance bounds :  0.007923257 0.8474102 
##   - best initial criterion value(s) :  10.72763 
## 
## N = 2, M = 5 machine precision = 2.22045e-16
## At X0, 0 variables are exactly at the bounds
## At iterate     0  f=      -10.728  |proj g|=       0.8044
## At iterate     1  f =      -10.957  |proj g|=        1.2817
## At iterate     2  f =      -13.556  |proj g|=        1.2252
## At iterate     3  f =      -15.227  |proj g|=        1.1513
## At iterate     4  f =       -15.37  |proj g|=       0.59625
## At iterate     5  f =      -15.379  |proj g|=       0.59258
## At iterate     6  f =      -15.422  |proj g|=        1.1185
## At iterate     7  f =      -15.477  |proj g|=        1.1044
## At iterate     8  f =      -15.513  |proj g|=       0.46151
## At iterate     9  f =      -15.517  |proj g|=        0.3865
## At iterate    10  f =      -15.521  |proj g|=       0.10777
## At iterate    11  f =      -15.521  |proj g|=      0.021783
## At iterate    12  f =      -15.521  |proj g|=    0.00051954
## At iterate    13  f =      -15.521  |proj g|=    0.00011162
## 
## iterations 13
## function evaluations 15
## segments explored during Cauchy searches 14
## BFGS updates skipped 0
## active bounds at final generalized Cauchy point 0
## norm of the final projected gradient 0.000111617
## final function value -15.5211
## 
## F = -15.5211
## final  value -15.521096 
## converged
## 
## optimisation start
## ------------------
## * estimation method   : MLE 
## * optimisation method : BFGS 
## * analytical gradient : used
## * trend model : ~1
## * covariance model : 
##   - type :  matern5_2 
##   - noise variances :
##  [1] 1e-05 1e-05 1e-05 1e-05 1e-05 1e-05 1e-05 1e-05 1e-05 1e-05 1e-05 1e-05
##   - parameters lower bounds :  1e-10 
##   - parameters upper bounds :  2 
##   - variance bounds :  0.009131837 1.021184 
##   - best initial criterion value(s) :  15.15918 
## 
## N = 2, M = 5 machine precision = 2.22045e-16
## At X0, 0 variables are exactly at the bounds
## At iterate     0  f=      -15.159  |proj g|=       1.9089
## At iterate     1  f =      -18.003  |proj g|=        1.8508
## At iterate     2  f =      -19.461  |proj g|=       0.88604
## At iterate     3  f =      -19.705  |proj g|=       0.85925
## At iterate     4  f =       -19.86  |proj g|=       0.82638
## At iterate     5  f =      -19.923  |proj g|=       0.79743
## At iterate     6  f =      -19.947  |proj g|=       0.42902
## At iterate     7  f =      -19.952  |proj g|=       0.26874
## At iterate     8  f =      -19.952  |proj g|=       0.15304
## At iterate     9  f =      -19.952  |proj g|=      0.009054
## At iterate    10  f =      -19.952  |proj g|=    0.00029395
## At iterate    11  f =      -19.952  |proj g|=    1.4205e-05
## 
## iterations 11
## function evaluations 14
## segments explored during Cauchy searches 12
## BFGS updates skipped 0
## active bounds at final generalized Cauchy point 0
## norm of the final projected gradient 1.42047e-05
## final function value -19.952
## 
## F = -19.952
## final  value -19.952047 
## converged
## 
## optimisation start
## ------------------
## * estimation method   : MLE 
## * optimisation method : BFGS 
## * analytical gradient : used
## * trend model : ~1
## * covariance model : 
##   - type :  matern5_2 
##   - noise variances :
##  [1] 1e-05 1e-05 1e-05 1e-05 1e-05 1e-05 1e-05 1e-05 1e-05 1e-05 1e-05 1e-05
##   - parameters lower bounds :  1e-10 
##   - parameters upper bounds :  2 
##   - variance bounds :  0.009131837 1.021184 
##   - best initial criterion value(s) :  15.15918 
## 
## N = 2, M = 5 machine precision = 2.22045e-16
## At X0, 0 variables are exactly at the bounds
## At iterate     0  f=      -15.159  |proj g|=       1.9089
## At iterate     1  f =      -18.003  |proj g|=        1.8508
## At iterate     2  f =      -19.461  |proj g|=       0.88604
## At iterate     3  f =      -19.705  |proj g|=       0.85925
## At iterate     4  f =       -19.86  |proj g|=       0.82638
## At iterate     5  f =      -19.923  |proj g|=       0.79743
## At iterate     6  f =      -19.947  |proj g|=       0.42902
## At iterate     7  f =      -19.952  |proj g|=       0.26874
## At iterate     8  f =      -19.952  |proj g|=       0.15304
## At iterate     9  f =      -19.952  |proj g|=      0.009054
## At iterate    10  f =      -19.952  |proj g|=    0.00029395
## At iterate    11  f =      -19.952  |proj g|=    1.4205e-05
## 
## iterations 11
## function evaluations 14
## segments explored during Cauchy searches 12
## BFGS updates skipped 0
## active bounds at final generalized Cauchy point 0
## norm of the final projected gradient 1.42047e-05
## final function value -19.952
## 
## F = -19.952
## final  value -19.952047 
## converged
## 
## optimisation start
## ------------------
## * estimation method   : MLE 
## * optimisation method : BFGS 
## * analytical gradient : used
## * trend model : ~1
## * covariance model : 
##   - type :  matern5_2 
##   - noise variances :
##  [1] 1e-05 1e-05 1e-05 1e-05 1e-05 1e-05 1e-05 1e-05 1e-05 1e-05 1e-05 1e-05 1e-05
## [14] 1e-05
##   - parameters lower bounds :  1e-10 
##   - parameters upper bounds :  2 
##   - variance bounds :  0.01043469 1.445253 
##   - best initial criterion value(s) :  22.48612 
## 
## N = 2, M = 5 machine precision = 2.22045e-16
## At X0, 0 variables are exactly at the bounds
## At iterate     0  f=      -22.486  |proj g|=         1.38
## At iterate     1  f =       -24.31  |proj g|=        1.8106
## At iterate     2  f =      -24.996  |proj g|=         1.785
## At iterate     3  f =      -25.403  |proj g|=        1.2628
## At iterate     4  f =      -25.437  |proj g|=        1.2572
## At iterate     5  f =      -25.488  |proj g|=        1.7413
## At iterate     6  f =      -25.548  |proj g|=        1.7308
## At iterate     7  f =      -25.574  |proj g|=       0.73451
## At iterate     8  f =      -25.578  |proj g|=       0.40364
## At iterate     9  f =      -25.579  |proj g|=      0.071179
## At iterate    10  f =      -25.579  |proj g|=     0.0053731
## At iterate    11  f =      -25.579  |proj g|=    0.00099242
## At iterate    12  f =      -25.579  |proj g|=    2.4984e-05
## 
## iterations 12
## function evaluations 16
## segments explored during Cauchy searches 13
## BFGS updates skipped 0
## active bounds at final generalized Cauchy point 0
## norm of the final projected gradient 2.4984e-05
## final function value -25.5793
## 
## F = -25.5793
## final  value -25.579348 
## converged
## 
## optimisation start
## ------------------
## * estimation method   : MLE 
## * optimisation method : BFGS 
## * analytical gradient : used
## * trend model : ~1
## * covariance model : 
##   - type :  matern5_2 
##   - noise variances :
##  [1] 1e-05 1e-05 1e-05 1e-05 1e-05 1e-05 1e-05 1e-05 1e-05 1e-05 1e-05 1e-05 1e-05
## [14] 1e-05
##   - parameters lower bounds :  1e-10 
##   - parameters upper bounds :  2 
##   - variance bounds :  0.01043469 1.445253 
##   - best initial criterion value(s) :  22.48612 
## 
## N = 2, M = 5 machine precision = 2.22045e-16
## At X0, 0 variables are exactly at the bounds
## At iterate     0  f=      -22.486  |proj g|=         1.38
## At iterate     1  f =       -24.31  |proj g|=        1.8106
## At iterate     2  f =      -24.996  |proj g|=         1.785
## At iterate     3  f =      -25.403  |proj g|=        1.2628
## At iterate     4  f =      -25.437  |proj g|=        1.2572
## At iterate     5  f =      -25.488  |proj g|=        1.7413
## At iterate     6  f =      -25.548  |proj g|=        1.7308
## At iterate     7  f =      -25.574  |proj g|=       0.73451
## At iterate     8  f =      -25.578  |proj g|=       0.40364
## At iterate     9  f =      -25.579  |proj g|=      0.071179
## At iterate    10  f =      -25.579  |proj g|=     0.0053731
## At iterate    11  f =      -25.579  |proj g|=    0.00099242
## At iterate    12  f =      -25.579  |proj g|=    2.4984e-05
## 
## iterations 12
## function evaluations 16
## segments explored during Cauchy searches 13
## BFGS updates skipped 0
## active bounds at final generalized Cauchy point 0
## norm of the final projected gradient 2.4984e-05
## final function value -25.5793
## 
## F = -25.5793
## final  value -25.579348 
## converged
</code></pre>
<pre><code class="language-r">  write.csv(dis,&quot;dis_pre.csv&quot;)
  write.csv(SD,&quot;OC_pre.csv&quot;)
</code></pre>
</div>
<script src="https://cdn.jsdelivr.net/npm/prismjs@1.29.0/components/prism-core.min.js" defer></script>
<script src="https://cdn.jsdelivr.net/npm/prismjs@1.29.0/plugins/autoloader/prism-autoloader.min.js" defer></script>
</body>
</html>
