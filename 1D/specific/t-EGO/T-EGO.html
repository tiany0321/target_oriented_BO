<!DOCTYPE html>
<html>
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">
<title></title>
<style type="text/css">
/**
 * Prism.s theme ported from highlight.js's xcode style
 */
pre code {
  padding: 1em;
}
.token.comment {
  color: #007400;
}
.token.punctuation {
  color: #999;
}
.token.tag,
.token.selector {
  color: #aa0d91;
}
.token.boolean,
.token.number,
.token.constant,
.token.symbol {
  color: #1c00cf;
}
.token.property,
.token.attr-name,
.token.string,
.token.char,
.token.builtin {
  color: #c41a16;
}
.token.inserted {
  background-color: #ccffd8;
}
.token.deleted {
  background-color: #ffebe9;
}
.token.operator,
.token.entity,
.token.url,
.language-css .token.string,
.style .token.string {
  color: #9a6e3a;
}
.token.atrule,
.token.attr-value,
.token.keyword {
  color: #836c28;
}
.token.function,
.token.class-name {
  color: #DD4A68;
}
.token.regex,
.token.important,
.token.variable {
  color: #5c2699;
}
.token.important,
.token.bold {
  font-weight: bold;
}
.token.italic {
  font-style: italic;
}
</style>
<style type="text/css">
body {
  font-family: sans-serif;
  max-width: 800px;
  margin: auto;
  padding: 1em;
  line-height: 1.5;
  box-sizing: border-box;
}
body, .footnotes, code { font-size: .9em; }
li li { font-size: .95em; }
*, *:before, *:after {
  box-sizing: inherit;
}
pre, img { max-width: 100%; }
pre, pre:hover {
  white-space: pre-wrap;
  word-break: break-all;
}
pre code {
  display: block;
  overflow-x: auto;
}
code { font-family: 'DejaVu Sans Mono', 'Droid Sans Mono', 'Lucida Console', Consolas, Monaco, monospace; }
:not(pre) > code, code[class] { background-color: #F8F8F8; }
code.language-undefined, pre > code:not([class]) {
  background-color: inherit;
  border: 1px solid #eee;
}
table {
  margin: auto;
  border-top: 1px solid #666;
}
table thead th { border-bottom: 1px solid #ddd; }
th, td { padding: 5px; }
thead, tfoot, tr:nth-child(even) { background: #eee; }
blockquote {
  color: #666;
  margin: 0;
  padding-left: 1em;
  border-left: 0.5em solid #eee;
}
hr, .footnotes::before { border: 1px dashed #ddd; }
.frontmatter { text-align: center; }
#TOC .numbered li { list-style: none; }
#TOC .numbered { padding-left: 0; }
#TOC .numbered ul { padding-left: 1em; }
table, .body h2 { border-bottom: 1px solid #666; }
.body .appendix, .appendix ~ h2 { border-bottom-style: dashed; }
.footnote-ref a::before { content: "["; }
.footnote-ref a::after { content: "]"; }
section.footnotes::before {
  content: "";
  display: block;
  max-width: 20em;
}

@media print {
  body {
    font-size: 12pt;
    max-width: 100%;
  }
  tr, img { page-break-inside: avoid; }
}
@media only screen and (min-width: 992px) {
  pre { white-space: pre; }
}
</style>
</head>
<body>
<div class="frontmatter">
<div class="title"><h1></h1></div>
<div class="author"><h2></h2></div>
<div class="date"><h3></h3></div>
</div>
<div class="body">
<pre><code class="language-r">setwd(&quot;C:\\Users\\12758\\Desktop\\target_oriented_BO\\1D\\specific\\t-EGO&quot;)
source(&quot;function_y.R&quot;)

estimator=3########t-EGO

##############create data
total.data.x &lt;- as.data.frame(seq(0, 1, , 200))
colnames(total.data.x)=&quot;x&quot;
write.csv(total.data.x,&quot;total.data.x.csv&quot;)

total.data.y &lt;- as.data.frame(f11_xiong(total.data.x))
colnames(total.data.y)=&quot;es&quot;
write.csv(total.data.y,&quot;total.data.y.csv&quot;)

order.data=as.data.frame(rep(1:nrow(total.data.x)))
colnames(order.data)=&quot;order.num&quot;
data.training=cbind(total.data.x,total.data.y,order.data)

############Set a target
tar=24
tar.points=data.training[which(abs(data.training[,&quot;es&quot;]-data.training[tar,&quot;es&quot;])&lt;=0.01*(max(data.training[,&quot;es&quot;])-min(data.training[,&quot;es&quot;]))),]
tar_data=data.training[tar,&quot;es&quot;]
write.csv(tar_data,&quot;tar_data.csv&quot;)

print(paste(&quot;The targeted y:&quot;, tar_data))
</code></pre>
<pre><code>## [1] &quot;The targeted y: -0.109781026409803&quot;
</code></pre>
<pre><code class="language-r">#############iteration number
iter=seq(0,10)
i=7

#########sample the training data
set.seed(11)
sample.num0&lt;-vector()
for(ii in 1:1000){
  s &lt;-  sample(1:(nrow(data.training)),40,replace=F,prob=NULL) 
  sample.num0&lt;-as.data.frame(rbind(sample.num0,s))
}

size=c(round(0.02*nrow(data.training)))
sample.num&lt;-sample.num0[,1:(size)]

split=sample.num[i,]
split=unlist(split)
training.data &lt;- data.training[split,]
write.csv(training.data,&quot;0train.csv&quot;)
training.data.x &lt;- data.frame(training.data[,&quot;x&quot;]) 
colnames(training.data.x)=&quot;x&quot;
training.data.y &lt;-  data.frame(training.data[,&quot;es&quot;]) 
colnames(training.data.y)=&quot;es&quot;

############virtual data
vir.total.data.x=data.frame(data.training[,&quot;x&quot;]) 
colnames(vir.total.data.x)=&quot;x&quot;
data.all=data.frame(data.training) 

virtual.data &lt;- data.training[-split,]
rownames(virtual.data)&lt;-1:nrow(virtual.data)
virtual.data.x=as.data.frame(virtual.data[,&quot;x&quot;])
colnames(virtual.data.x)=&quot;x&quot;

##########km model prediction
gp = fn.gp(training.data.x, training.data.y,virtual.data.x)
</code></pre>
<pre><code>## 
## optimisation start
## ------------------
## * estimation method   : MLE 
## * optimisation method : BFGS 
## * analytical gradient : used
## * trend model : ~1
## * covariance model : 
##   - type :  matern5_2 
##   - noise variances :
## [1] 1e-05 1e-05 1e-05 1e-05
##   - parameters lower bounds :  1e-10 
##   - parameters upper bounds :  1.005025 
##   - variance bounds :  0.002404243 0.3060814 
##   - best initial criterion value(s) :  2.460662 
## 
## N = 2, M = 5 machine precision = 2.22045e-16
## At X0, 0 variables are exactly at the bounds
## At iterate     0  f=      -2.4607  |proj g|=      0.96275
## At iterate     1  f =      -2.6674  |proj g|=       0.91789
## At iterate     2  f =      -2.7745  |proj g|=      0.097813
## At iterate     3  f =      -2.7792  |proj g|=       0.28285
## At iterate     4  f =      -2.7822  |proj g|=      0.093416
## At iterate     5  f =      -2.7825  |proj g|=       0.02129
## At iterate     6  f =      -2.7825  |proj g|=     0.0025827
## At iterate     7  f =      -2.7825  |proj g|=    0.00023432
## 
## iterations 7
## function evaluations 11
## segments explored during Cauchy searches 8
## BFGS updates skipped 0
## active bounds at final generalized Cauchy point 0
## norm of the final projected gradient 0.000234318
## final function value -2.78246
## 
## F = -2.78246
## final  value -2.782457 
## converged
</code></pre>
<pre><code class="language-r">gp.p = fn.gp(training.data.x, training.data.y,vir.total.data.x)
</code></pre>
<pre><code>## 
## optimisation start
## ------------------
## * estimation method   : MLE 
## * optimisation method : BFGS 
## * analytical gradient : used
## * trend model : ~1
## * covariance model : 
##   - type :  matern5_2 
##   - noise variances :
## [1] 1e-05 1e-05 1e-05 1e-05
##   - parameters lower bounds :  1e-10 
##   - parameters upper bounds :  1.005025 
##   - variance bounds :  0.002404243 0.3060814 
##   - best initial criterion value(s) :  2.460662 
## 
## N = 2, M = 5 machine precision = 2.22045e-16
## At X0, 0 variables are exactly at the bounds
## At iterate     0  f=      -2.4607  |proj g|=      0.96275
## At iterate     1  f =      -2.6674  |proj g|=       0.91789
## At iterate     2  f =      -2.7745  |proj g|=      0.097813
## At iterate     3  f =      -2.7792  |proj g|=       0.28285
## At iterate     4  f =      -2.7822  |proj g|=      0.093416
## At iterate     5  f =      -2.7825  |proj g|=       0.02129
## At iterate     6  f =      -2.7825  |proj g|=     0.0025827
## At iterate     7  f =      -2.7825  |proj g|=    0.00023432
## 
## iterations 7
## function evaluations 11
## segments explored during Cauchy searches 8
## BFGS updates skipped 0
## active bounds at final generalized Cauchy point 0
## norm of the final projected gradient 0.000234318
## final function value -2.78246
## 
## F = -2.78246
## final  value -2.782457 
## converged
</code></pre>
<pre><code class="language-r">mean=as.data.frame(gp.p[,1])
mean=unlist(mean)
t=total.data.x
t=unlist(t)
yy0=cbind(total.data.x,as.data.frame(mean),as.data.frame(gp.p[,1]-gp.p[,2]),as.data.frame(gp.p[,1]+gp.p[,2]))
colnames(yy0)=c(&quot;x&quot;,&quot;mean&quot;,&quot;lower&quot;,&quot;upper&quot;)
write.csv(yy0,&quot;yy0.csv&quot;)
head(yy0)
</code></pre>
<pre><code>##                      x      mean      lower     upper
## gp.p[, 1]1 0.000000000 0.2140459 0.06028518 0.3678065
## gp.p[, 1]2 0.005025126 0.2140240 0.06026335 0.3677846
## gp.p[, 1]3 0.010050251 0.2139998 0.06023925 0.3677604
## gp.p[, 1]4 0.015075377 0.2139732 0.06021265 0.3677337
## gp.p[, 1]5 0.020100503 0.2139438 0.06018332 0.3677042
## gp.p[, 1]6 0.025125628 0.2139113 0.06015098 0.3676716
</code></pre>
<pre><code class="language-r">##########data frame
data.OC.SD=matrix(,length(iter)-1,1)
data.dis=matrix(,length(iter)-1,1)
SD=matrix(0,length(iter),1)
dis=matrix(0,length(iter),1)

########utility value
EI= fn.utility(data.training,estimator,training.data,gp.p,tar_num = tar)
EI[split,]=0
EI=na.omit(EI)
ee0=cbind(total.data.x,EI)
write.csv(ee0,&quot;ee0.csv&quot;)

EI=as.data.frame(EI)

SD[1,1]= fn.OC(data.training,training.data)
dis[1,1]= fn.OC(data.training,training.data)

######recommend the option
num= which(EI[,&quot;EI&quot;]==max(EI[,&quot;EI&quot;]))

u=1
pic0=rep(1:10)
pic0=as.character(pic0)
pic1=rep(&quot;csv&quot;,10)
pic=paste(pic0,pic1,sep=&quot;.&quot;)
er0=rep(1:10)
er0=as.character(er0)
er1=rep(&quot;csv&quot;,10)
er=paste(er0,er1,sep=&quot;ee.&quot;)
train0=rep(1:10)
train0=as.character(train0)
train1=rep(&quot;csv&quot;,10)
train=paste(train0,train1,sep=&quot;train.&quot;)

num.count=c(split,num)
training.data0=training.data
virtual.data0=virtual.data

####start iteration
u=1
repeat{
  ########updated training data and virtual data
  training.data0[nrow(training.data0)+1,]=virtual.data0[which(virtual.data0[,&quot;order.num&quot;]==num),]
  virtual.data0=virtual.data0[-which(virtual.data0[,&quot;order.num&quot;]==num),]
  rownames(training.data0)&lt;-1:nrow(training.data0)
  rownames(virtual.data0)&lt;-1:nrow(virtual.data0)
  
  training.data0.x &lt;- data.frame(training.data0[,&quot;x&quot;])
  colnames(training.data0.x)=&quot;x&quot;
  training.data0.y &lt;-  data.frame(training.data0[,&quot;es&quot;]) 
  colnames(training.data0.y)=&quot;es&quot;
  virtual.data0.x=as.data.frame(virtual.data0[,&quot;x&quot;])
  colnames(virtual.data0.x)=&quot;x&quot;
  
  ##########km model prediction
  gp0 = fn.gp(training.data0.x, training.data0.y,virtual.data0.x)
  gp0.p = fn.gp(training.data0.x, training.data0.y,vir.total.data.x)
  
  
  ########utility value
  EI=fn.utility(data.training,estimator,training.data0,gp0.p,tar_num = tar)
  EI=as.data.frame(EI)
  EI[num.count,]=0
  EI=na.omit(EI)
  
  ######recommend the option
  num= which(EI[,&quot;EI&quot;]==max(EI[,&quot;EI&quot;]))
  mean=as.data.frame(gp0.p[,1])
  mean=unlist(mean)
  t=total.data.x
  t=unlist(t)
  
  SD[u+1,]=fn.OC(data.training,training.data0)
  dis[u+1,]=fn.dis(data.training, training.data0[nrow(training.data0),&quot;es&quot;])
  
  if(max(EI)==0)
  {break}
  
  if (u==1|u==2|u==3|u==4|u==5|u==6)
  { 
    yy0=cbind(total.data.x,as.data.frame(mean),as.data.frame(gp0.p[,1]-gp0.p[,2]),as.data.frame(gp0.p[,1]+gp0.p[,2]))
    colnames(yy0)=c(&quot;x&quot;,&quot;mean&quot;,&quot;lower&quot;,&quot;upper&quot;)
    write.csv(yy0,pic[u])
    ee0=cbind(total.data.x,EI)
    write.csv(ee0,er[u])
    write.csv(training.data0,train[u])
  }
  num.count=c(num.count,num)
  #######stopping criteria
  if(u&gt;=length(iter)-1)
    break
  
  #######points in the the tolerance of t
  if(min(abs(training.data0[,&quot;es&quot;]-data.training[tar,&quot;es&quot;]))&lt;=0.01*(max(data.training[,&quot;es&quot;])-min(data.training[,&quot;es&quot;])))
  {
    repeat{ 
      u=u+1
      yy0=cbind(total.data.x,as.data.frame(mean),as.data.frame(gp0.p[,1]-gp0.p[,2]),as.data.frame(gp0.p[,1]+gp0.p[,2]))
      colnames(yy0)=c(&quot;x&quot;,&quot;mean&quot;,&quot;lower&quot;,&quot;upper&quot;)
      write.csv(yy0,pic[u])
      ee0=cbind(total.data.x,EI)
      write.csv(ee0,er[u])
      training.data0=rbind(training.data0,training.data0[nrow(training.data0),])
      write.csv(training.data0,train[u])
      if(u&gt;6)
        break
    }
  }
  u=u+1
  
}
</code></pre>
<pre><code>## 
## optimisation start
## ------------------
## * estimation method   : MLE 
## * optimisation method : BFGS 
## * analytical gradient : used
## * trend model : ~1
## * covariance model : 
##   - type :  matern5_2 
##   - noise variances :
## [1] 1e-05 1e-05 1e-05 1e-05 1e-05
##   - parameters lower bounds :  1e-10 
##   - parameters upper bounds :  1.18593 
##   - variance bounds :  0.007449175 1.086683 
##   - best initial criterion value(s) :  -0.6516522 
## 
## N = 2, M = 5 machine precision = 2.22045e-16
## At X0, 0 variables are exactly at the bounds
## At iterate     0  f=      0.65165  |proj g|=       1.1319
## At iterate     1  f =      0.46365  |proj g|=       0.97958
## At iterate     2  f =      0.46061  |proj g|=       0.49937
## At iterate     3  f =       0.4591  |proj g|=       0.10004
## At iterate     4  f =      0.45907  |proj g|=      0.004687
## At iterate     5  f =      0.45907  |proj g|=    9.4061e-05
## At iterate     6  f =      0.45907  |proj g|=    8.2407e-07
## 
## iterations 6
## function evaluations 9
## segments explored during Cauchy searches 7
## BFGS updates skipped 0
## active bounds at final generalized Cauchy point 0
## norm of the final projected gradient 8.24075e-07
## final function value 0.459067
## 
## F = 0.459067
## final  value 0.459067 
## converged
## 
## optimisation start
## ------------------
## * estimation method   : MLE 
## * optimisation method : BFGS 
## * analytical gradient : used
## * trend model : ~1
## * covariance model : 
##   - type :  matern5_2 
##   - noise variances :
## [1] 1e-05 1e-05 1e-05 1e-05 1e-05
##   - parameters lower bounds :  1e-10 
##   - parameters upper bounds :  1.18593 
##   - variance bounds :  0.007449175 1.086683 
##   - best initial criterion value(s) :  -0.6516522 
## 
## N = 2, M = 5 machine precision = 2.22045e-16
## At X0, 0 variables are exactly at the bounds
## At iterate     0  f=      0.65165  |proj g|=       1.1319
## At iterate     1  f =      0.46365  |proj g|=       0.97958
## At iterate     2  f =      0.46061  |proj g|=       0.49937
## At iterate     3  f =       0.4591  |proj g|=       0.10004
## At iterate     4  f =      0.45907  |proj g|=      0.004687
## At iterate     5  f =      0.45907  |proj g|=    9.4061e-05
## At iterate     6  f =      0.45907  |proj g|=    8.2407e-07
## 
## iterations 6
## function evaluations 9
## segments explored during Cauchy searches 7
## BFGS updates skipped 0
## active bounds at final generalized Cauchy point 0
## norm of the final projected gradient 8.24075e-07
## final function value 0.459067
## 
## F = 0.459067
## final  value 0.459067 
## converged
## 
## optimisation start
## ------------------
## * estimation method   : MLE 
## * optimisation method : BFGS 
## * analytical gradient : used
## * trend model : ~1
## * covariance model : 
##   - type :  matern5_2 
##   - noise variances :
## [1] 1e-05 1e-05 1e-05 1e-05 1e-05 1e-05
##   - parameters lower bounds :  1e-10 
##   - parameters upper bounds :  1.18593 
##   - variance bounds :  0.005671373 0.8985184 
##   - best initial criterion value(s) :  0.230909 
## 
## N = 2, M = 5 machine precision = 2.22045e-16
## At X0, 0 variables are exactly at the bounds
## At iterate     0  f=     -0.23091  |proj g|=       1.1319
## At iterate     1  f =     -0.95657  |proj g|=       0.81439
## At iterate     2  f =      -1.1211  |proj g|=        1.0644
## At iterate     3  f =      -1.2512  |proj g|=       0.98738
## At iterate     4  f =      -1.3449  |proj g|=       0.73871
## At iterate     5  f =      -1.3893  |proj g|=       0.70693
## At iterate     6  f =       -1.416  |proj g|=       0.67538
## At iterate     7  f =      -1.4251  |proj g|=       0.97821
## At iterate     8  f =      -1.4365  |proj g|=       0.12996
## At iterate     9  f =      -1.4376  |proj g|=       0.20713
## At iterate    10  f =      -1.4379  |proj g|=      0.084893
## At iterate    11  f =      -1.4379  |proj g|=     0.0065246
## At iterate    12  f =      -1.4379  |proj g|=    5.8003e-05
## At iterate    13  f =      -1.4379  |proj g|=    8.0898e-06
## 
## iterations 13
## function evaluations 16
## segments explored during Cauchy searches 14
## BFGS updates skipped 0
## active bounds at final generalized Cauchy point 0
## norm of the final projected gradient 8.08979e-06
## final function value -1.4379
## 
## F = -1.4379
## final  value -1.437904 
## converged
## 
## optimisation start
## ------------------
## * estimation method   : MLE 
## * optimisation method : BFGS 
## * analytical gradient : used
## * trend model : ~1
## * covariance model : 
##   - type :  matern5_2 
##   - noise variances :
## [1] 1e-05 1e-05 1e-05 1e-05 1e-05 1e-05
##   - parameters lower bounds :  1e-10 
##   - parameters upper bounds :  1.18593 
##   - variance bounds :  0.005671373 0.8985184 
##   - best initial criterion value(s) :  0.230909 
## 
## N = 2, M = 5 machine precision = 2.22045e-16
## At X0, 0 variables are exactly at the bounds
## At iterate     0  f=     -0.23091  |proj g|=       1.1319
## At iterate     1  f =     -0.95657  |proj g|=       0.81439
## At iterate     2  f =      -1.1211  |proj g|=        1.0644
## At iterate     3  f =      -1.2512  |proj g|=       0.98738
## At iterate     4  f =      -1.3449  |proj g|=       0.73871
## At iterate     5  f =      -1.3893  |proj g|=       0.70693
## At iterate     6  f =       -1.416  |proj g|=       0.67538
## At iterate     7  f =      -1.4251  |proj g|=       0.97821
## At iterate     8  f =      -1.4365  |proj g|=       0.12996
## At iterate     9  f =      -1.4376  |proj g|=       0.20713
## At iterate    10  f =      -1.4379  |proj g|=      0.084893
## At iterate    11  f =      -1.4379  |proj g|=     0.0065246
## At iterate    12  f =      -1.4379  |proj g|=    5.8003e-05
## At iterate    13  f =      -1.4379  |proj g|=    8.0898e-06
## 
## iterations 13
## function evaluations 16
## segments explored during Cauchy searches 14
## BFGS updates skipped 0
## active bounds at final generalized Cauchy point 0
## norm of the final projected gradient 8.08979e-06
## final function value -1.4379
## 
## F = -1.4379
## final  value -1.437904 
## converged
## 
## optimisation start
## ------------------
## * estimation method   : MLE 
## * optimisation method : BFGS 
## * analytical gradient : used
## * trend model : ~1
## * covariance model : 
##   - type :  matern5_2 
##   - noise variances :
## [1] 1e-05 1e-05 1e-05 1e-05 1e-05 1e-05 1e-05
##   - parameters lower bounds :  1e-10 
##   - parameters upper bounds :  1.18593 
##   - variance bounds :  0.004987607 0.7787627 
##   - best initial criterion value(s) :  3.110232 
## 
## N = 2, M = 5 machine precision = 2.22045e-16
## At X0, 0 variables are exactly at the bounds
## At iterate     0  f=      -3.1102  |proj g|=       1.1319
## At iterate     1  f =      -4.7635  |proj g|=       0.70629
## At iterate     2  f =      -5.2064  |proj g|=        1.0517
## At iterate     3  f =      -5.4213  |proj g|=        1.0346
## At iterate     4  f =      -5.6181  |proj g|=        0.6253
## At iterate     5  f =      -5.7159  |proj g|=       0.59051
## At iterate     6  f =      -5.7785  |proj g|=       0.55118
## At iterate     7  f =      -5.8016  |proj g|=       0.96673
## At iterate     8  f =       -5.831  |proj g|=       0.27377
## At iterate     9  f =      -5.8347  |proj g|=       0.25379
## At iterate    10  f =      -5.8362  |proj g|=       0.25844
## At iterate    11  f =      -5.8365  |proj g|=      0.054514
## At iterate    12  f =      -5.8365  |proj g|=     0.0028278
## At iterate    13  f =      -5.8365  |proj g|=    6.6952e-05
## At iterate    14  f =      -5.8365  |proj g|=    9.0547e-05
## 
## iterations 14
## function evaluations 17
## segments explored during Cauchy searches 15
## BFGS updates skipped 0
## active bounds at final generalized Cauchy point 0
## norm of the final projected gradient 9.05475e-05
## final function value -5.83652
## 
## F = -5.83652
## final  value -5.836521 
## converged
## 
## optimisation start
## ------------------
## * estimation method   : MLE 
## * optimisation method : BFGS 
## * analytical gradient : used
## * trend model : ~1
## * covariance model : 
##   - type :  matern5_2 
##   - noise variances :
## [1] 1e-05 1e-05 1e-05 1e-05 1e-05 1e-05 1e-05
##   - parameters lower bounds :  1e-10 
##   - parameters upper bounds :  1.18593 
##   - variance bounds :  0.004987607 0.7787627 
##   - best initial criterion value(s) :  3.110232 
## 
## N = 2, M = 5 machine precision = 2.22045e-16
## At X0, 0 variables are exactly at the bounds
## At iterate     0  f=      -3.1102  |proj g|=       1.1319
## At iterate     1  f =      -4.7635  |proj g|=       0.70629
## At iterate     2  f =      -5.2064  |proj g|=        1.0517
## At iterate     3  f =      -5.4213  |proj g|=        1.0346
## At iterate     4  f =      -5.6181  |proj g|=        0.6253
## At iterate     5  f =      -5.7159  |proj g|=       0.59051
## At iterate     6  f =      -5.7785  |proj g|=       0.55118
## At iterate     7  f =      -5.8016  |proj g|=       0.96673
## At iterate     8  f =       -5.831  |proj g|=       0.27377
## At iterate     9  f =      -5.8347  |proj g|=       0.25379
## At iterate    10  f =      -5.8362  |proj g|=       0.25844
## At iterate    11  f =      -5.8365  |proj g|=      0.054514
## At iterate    12  f =      -5.8365  |proj g|=     0.0028278
## At iterate    13  f =      -5.8365  |proj g|=    6.6952e-05
## At iterate    14  f =      -5.8365  |proj g|=    9.0547e-05
## 
## iterations 14
## function evaluations 17
## segments explored during Cauchy searches 15
## BFGS updates skipped 0
## active bounds at final generalized Cauchy point 0
## norm of the final projected gradient 9.05475e-05
## final function value -5.83652
## 
## F = -5.83652
## final  value -5.836521 
## converged
## 
## optimisation start
## ------------------
## * estimation method   : MLE 
## * optimisation method : BFGS 
## * analytical gradient : used
## * trend model : ~1
## * covariance model : 
##   - type :  matern5_2 
##   - noise variances :
##  [1] 1e-05 1e-05 1e-05 1e-05 1e-05 1e-05 1e-05 1e-05 1e-05 1e-05 1e-05 1e-05
##   - parameters lower bounds :  1e-10 
##   - parameters upper bounds :  1.336683 
##   - variance bounds :  0.004696069 0.6517812 
##   - best initial criterion value(s) :  22.56465 
## 
## N = 2, M = 5 machine precision = 2.22045e-16
## At X0, 0 variables are exactly at the bounds
## At iterate     0  f=      -22.565  |proj g|=       1.2758
## At iterate     1  f =      -24.251  |proj g|=       0.58747
## At iterate     2  f =      -24.914  |proj g|=        1.1891
## At iterate     3  f =      -25.245  |proj g|=       0.53544
## At iterate     4  f =      -25.511  |proj g|=       0.49867
## At iterate     5  f =      -25.662  |proj g|=       0.46083
## At iterate     6  f =      -25.777  |proj g|=       0.41691
## At iterate     7  f =      -25.826  |proj g|=        1.0932
## At iterate     8  f =        -25.9  |proj g|=       0.34498
## At iterate     9  f =      -25.941  |proj g|=       0.29847
## At iterate    10  f =      -25.958  |proj g|=       0.30995
## At iterate    11  f =      -25.967  |proj g|=        0.3181
## At iterate    12  f =       -25.97  |proj g|=       0.13558
## At iterate    13  f =       -25.97  |proj g|=      0.071928
## At iterate    14  f =       -25.97  |proj g|=     0.0078915
## At iterate    15  f =       -25.97  |proj g|=    0.00081012
## At iterate    16  f =       -25.97  |proj g|=     3.807e-05
## At iterate    17  f =       -25.97  |proj g|=    3.5558e-08
## 
## iterations 17
## function evaluations 22
## segments explored during Cauchy searches 18
## BFGS updates skipped 0
## active bounds at final generalized Cauchy point 0
## norm of the final projected gradient 3.55584e-08
## final function value -25.9701
## 
## F = -25.9701
## final  value -25.970144 
## converged
## 
## optimisation start
## ------------------
## * estimation method   : MLE 
## * optimisation method : BFGS 
## * analytical gradient : used
## * trend model : ~1
## * covariance model : 
##   - type :  matern5_2 
##   - noise variances :
##  [1] 1e-05 1e-05 1e-05 1e-05 1e-05 1e-05 1e-05 1e-05 1e-05 1e-05 1e-05 1e-05
##   - parameters lower bounds :  1e-10 
##   - parameters upper bounds :  1.336683 
##   - variance bounds :  0.004696069 0.6517812 
##   - best initial criterion value(s) :  22.56465 
## 
## N = 2, M = 5 machine precision = 2.22045e-16
## At X0, 0 variables are exactly at the bounds
## At iterate     0  f=      -22.565  |proj g|=       1.2758
## At iterate     1  f =      -24.251  |proj g|=       0.58747
## At iterate     2  f =      -24.914  |proj g|=        1.1891
## At iterate     3  f =      -25.245  |proj g|=       0.53544
## At iterate     4  f =      -25.511  |proj g|=       0.49867
## At iterate     5  f =      -25.662  |proj g|=       0.46083
## At iterate     6  f =      -25.777  |proj g|=       0.41691
## At iterate     7  f =      -25.826  |proj g|=        1.0932
## At iterate     8  f =        -25.9  |proj g|=       0.34498
## At iterate     9  f =      -25.941  |proj g|=       0.29847
## At iterate    10  f =      -25.958  |proj g|=       0.30995
## At iterate    11  f =      -25.967  |proj g|=        0.3181
## At iterate    12  f =       -25.97  |proj g|=       0.13558
## At iterate    13  f =       -25.97  |proj g|=      0.071928
## At iterate    14  f =       -25.97  |proj g|=     0.0078915
## At iterate    15  f =       -25.97  |proj g|=    0.00081012
## At iterate    16  f =       -25.97  |proj g|=     3.807e-05
## At iterate    17  f =       -25.97  |proj g|=    3.5558e-08
## 
## iterations 17
## function evaluations 22
## segments explored during Cauchy searches 18
## BFGS updates skipped 0
## active bounds at final generalized Cauchy point 0
## norm of the final projected gradient 3.55584e-08
## final function value -25.9701
## 
## F = -25.9701
## final  value -25.970144 
## converged
## 
## optimisation start
## ------------------
## * estimation method   : MLE 
## * optimisation method : BFGS 
## * analytical gradient : used
## * trend model : ~1
## * covariance model : 
##   - type :  matern5_2 
##   - noise variances :
##  [1] 1e-05 1e-05 1e-05 1e-05 1e-05 1e-05 1e-05 1e-05 1e-05 1e-05 1e-05 1e-05 1e-05
## [14] 1e-05
##   - parameters lower bounds :  1e-10 
##   - parameters upper bounds :  2 
##   - variance bounds :  0.006573963 0.7491079 
##   - best initial criterion value(s) :  26.65507 
## 
## N = 2, M = 5 machine precision = 2.22045e-16
## At X0, 0 variables are exactly at the bounds
## At iterate     0  f=      -26.655  |proj g|=       1.9089
## At iterate     1  f =      -28.315  |proj g|=         1.869
## At iterate     2  f =      -29.902  |proj g|=        1.1872
## At iterate     3  f =      -30.107  |proj g|=        1.6342
## At iterate     4  f =      -30.183  |proj g|=       0.54252
## At iterate     5  f =      -30.215  |proj g|=       0.51674
## At iterate     6  f =      -30.221  |proj g|=       0.23535
## At iterate     7  f =      -30.222  |proj g|=      0.067825
## At iterate     8  f =      -30.222  |proj g|=      0.055689
## At iterate     9  f =      -30.222  |proj g|=    0.00038442
## At iterate    10  f =      -30.222  |proj g|=    7.8156e-06
## 
## iterations 10
## function evaluations 12
## segments explored during Cauchy searches 11
## BFGS updates skipped 0
## active bounds at final generalized Cauchy point 0
## norm of the final projected gradient 7.81563e-06
## final function value -30.2218
## 
## F = -30.2218
## final  value -30.221805 
## converged
## 
## optimisation start
## ------------------
## * estimation method   : MLE 
## * optimisation method : BFGS 
## * analytical gradient : used
## * trend model : ~1
## * covariance model : 
##   - type :  matern5_2 
##   - noise variances :
##  [1] 1e-05 1e-05 1e-05 1e-05 1e-05 1e-05 1e-05 1e-05 1e-05 1e-05 1e-05 1e-05 1e-05
## [14] 1e-05
##   - parameters lower bounds :  1e-10 
##   - parameters upper bounds :  2 
##   - variance bounds :  0.006573963 0.7491079 
##   - best initial criterion value(s) :  26.65507 
## 
## N = 2, M = 5 machine precision = 2.22045e-16
## At X0, 0 variables are exactly at the bounds
## At iterate     0  f=      -26.655  |proj g|=       1.9089
## At iterate     1  f =      -28.315  |proj g|=         1.869
## At iterate     2  f =      -29.902  |proj g|=        1.1872
## At iterate     3  f =      -30.107  |proj g|=        1.6342
## At iterate     4  f =      -30.183  |proj g|=       0.54252
## At iterate     5  f =      -30.215  |proj g|=       0.51674
## At iterate     6  f =      -30.221  |proj g|=       0.23535
## At iterate     7  f =      -30.222  |proj g|=      0.067825
## At iterate     8  f =      -30.222  |proj g|=      0.055689
## At iterate     9  f =      -30.222  |proj g|=    0.00038442
## At iterate    10  f =      -30.222  |proj g|=    7.8156e-06
## 
## iterations 10
## function evaluations 12
## segments explored during Cauchy searches 11
## BFGS updates skipped 0
## active bounds at final generalized Cauchy point 0
## norm of the final projected gradient 7.81563e-06
## final function value -30.2218
## 
## F = -30.2218
## final  value -30.221805 
## converged
</code></pre>
<pre><code class="language-r">write.csv(dis,&quot;dis_pre.csv&quot;)
write.csv(SD,&quot;OC_pre.csv&quot;)
</code></pre>
</div>
<script src="https://cdn.jsdelivr.net/npm/prismjs@1.29.0/components/prism-core.min.js" defer></script>
<script src="https://cdn.jsdelivr.net/npm/prismjs@1.29.0/plugins/autoloader/prism-autoloader.min.js" defer></script>
</body>
</html>
