<!DOCTYPE html>
<html>
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">
<title></title>
<style type="text/css">
/**
 * Prism.s theme ported from highlight.js's xcode style
 */
pre code {
  padding: 1em;
}
.token.comment {
  color: #007400;
}
.token.punctuation {
  color: #999;
}
.token.tag,
.token.selector {
  color: #aa0d91;
}
.token.boolean,
.token.number,
.token.constant,
.token.symbol {
  color: #1c00cf;
}
.token.property,
.token.attr-name,
.token.string,
.token.char,
.token.builtin {
  color: #c41a16;
}
.token.inserted {
  background-color: #ccffd8;
}
.token.deleted {
  background-color: #ffebe9;
}
.token.operator,
.token.entity,
.token.url,
.language-css .token.string,
.style .token.string {
  color: #9a6e3a;
}
.token.atrule,
.token.attr-value,
.token.keyword {
  color: #836c28;
}
.token.function,
.token.class-name {
  color: #DD4A68;
}
.token.regex,
.token.important,
.token.variable {
  color: #5c2699;
}
.token.important,
.token.bold {
  font-weight: bold;
}
.token.italic {
  font-style: italic;
}
</style>
<style type="text/css">
body {
  font-family: sans-serif;
  max-width: 800px;
  margin: auto;
  padding: 1em;
  line-height: 1.5;
  box-sizing: border-box;
}
body, .footnotes, code { font-size: .9em; }
li li { font-size: .95em; }
*, *:before, *:after {
  box-sizing: inherit;
}
pre, img { max-width: 100%; }
pre, pre:hover {
  white-space: pre-wrap;
  word-break: break-all;
}
pre code {
  display: block;
  overflow-x: auto;
}
code { font-family: 'DejaVu Sans Mono', 'Droid Sans Mono', 'Lucida Console', Consolas, Monaco, monospace; }
:not(pre) > code, code[class] { background-color: #F8F8F8; }
code.language-undefined, pre > code:not([class]) {
  background-color: inherit;
  border: 1px solid #eee;
}
table {
  margin: auto;
  border-top: 1px solid #666;
}
table thead th { border-bottom: 1px solid #ddd; }
th, td { padding: 5px; }
thead, tfoot, tr:nth-child(even) { background: #eee; }
blockquote {
  color: #666;
  margin: 0;
  padding-left: 1em;
  border-left: 0.5em solid #eee;
}
hr, .footnotes::before { border: 1px dashed #ddd; }
.frontmatter { text-align: center; }
#TOC .numbered li { list-style: none; }
#TOC .numbered { padding-left: 0; }
#TOC .numbered ul { padding-left: 1em; }
table, .body h2 { border-bottom: 1px solid #666; }
.body .appendix, .appendix ~ h2 { border-bottom-style: dashed; }
.footnote-ref a::before { content: "["; }
.footnote-ref a::after { content: "]"; }
section.footnotes::before {
  content: "";
  display: block;
  max-width: 20em;
}

@media print {
  body {
    font-size: 12pt;
    max-width: 100%;
  }
  tr, img { page-break-inside: avoid; }
}
@media only screen and (min-width: 992px) {
  pre { white-space: pre; }
}
</style>
</head>
<body>
<div class="frontmatter">
<div class="title"><h1></h1></div>
<div class="author"><h2></h2></div>
<div class="date"><h3></h3></div>
</div>
<div class="body">
<pre><code class="language-r">setwd(&quot;C:\\Users\\12758\\Desktop\\target_oriented_BO\\1D\\specific\\MOAF&quot;)
source(&quot;function_abs_y-t.R&quot;)

estimator=5########MOAF

##############create data
total.data.x &lt;- as.data.frame(seq(0, 1, , 200))
colnames(total.data.x)=&quot;x&quot;
write.csv(total.data.x,&quot;total.data.x.csv&quot;)

total.data.y &lt;- as.data.frame(f11_xiong(total.data.x))
colnames(total.data.y)=&quot;es&quot;
order.data=as.data.frame(rep(1:nrow(total.data.x)))
colnames(order.data)=&quot;order.num&quot;
data.training=cbind(total.data.x,total.data.y,order.data)

############Set a target
tar=24
tar.points=data.training[which(abs(data.training[,&quot;es&quot;]-data.training[tar,&quot;es&quot;])&lt;=0.01*(max(data.training[,&quot;es&quot;])-min(data.training[,&quot;es&quot;]))),]
tar_data=data.training[tar,&quot;es&quot;]
data.training$pro.es=abs(data.training$es-tar_data)
write.csv(data.training$pro.es,&quot;total.data.y.csv&quot;)
tar_data=min(data.training$pro.es)
write.csv(tar_data,&quot;tar_data.csv&quot;)

print(paste(&quot;The targeted y:&quot;, tar_data))
</code></pre>
<pre><code>## [1] &quot;The targeted y: 0&quot;
</code></pre>
<pre><code class="language-r">#############iteration number
iter=seq(0,10)
i=7

#########sample the training data
set.seed(11)
sample.num0&lt;-vector()
for(ii in 1:1000){
  s &lt;-  sample(1:(nrow(data.training)),40,replace=F,prob=NULL) 
  sample.num0&lt;-as.data.frame(rbind(sample.num0,s))
}

size=c(round(0.02*nrow(data.training)))
sample.num&lt;-sample.num0[,1:(size)]

split=sample.num[i,]
split=unlist(split)
training.data &lt;- data.training[split,]
write.csv(training.data,&quot;0train.csv&quot;)
training.data.x &lt;- data.frame(training.data[,&quot;x&quot;]) 
colnames(training.data.x)=&quot;x&quot;
training.data.y &lt;-  data.frame(training.data[,&quot;pro.es&quot;]) 
colnames(training.data.y)=&quot;pro.es&quot;

############virtual data
vir.total.data.x=data.frame(data.training[,&quot;x&quot;]) 
colnames(vir.total.data.x)=&quot;x&quot;
data.all=data.frame(data.training) 

virtual.data &lt;- data.training[-split,]
rownames(virtual.data)&lt;-1:nrow(virtual.data)
virtual.data.x=as.data.frame(virtual.data[,&quot;x&quot;])
colnames(virtual.data.x)=&quot;x&quot;

##########km model prediction
gp = fn.gp(training.data.x, training.data.y,virtual.data.x)
</code></pre>
<pre><code>## 
## optimisation start
## ------------------
## * estimation method   : MLE 
## * optimisation method : BFGS 
## * analytical gradient : used
## * trend model : ~1
## * covariance model : 
##   - type :  matern5_2 
##   - noise variances :
## [1] 1e-05 1e-05 1e-05 1e-05
##   - parameters lower bounds :  1e-10 
##   - parameters upper bounds :  1.005025 
##   - variance bounds :  0.002404243 0.3060814 
##   - best initial criterion value(s) :  2.460662 
## 
## N = 2, M = 5 machine precision = 2.22045e-16
## At X0, 0 variables are exactly at the bounds
## At iterate     0  f=      -2.4607  |proj g|=      0.96275
## At iterate     1  f =      -2.6674  |proj g|=       0.91789
## At iterate     2  f =      -2.7745  |proj g|=      0.097813
## At iterate     3  f =      -2.7792  |proj g|=       0.28285
## At iterate     4  f =      -2.7822  |proj g|=      0.093416
## At iterate     5  f =      -2.7825  |proj g|=       0.02129
## At iterate     6  f =      -2.7825  |proj g|=     0.0025827
## At iterate     7  f =      -2.7825  |proj g|=    0.00023432
## 
## iterations 7
## function evaluations 11
## segments explored during Cauchy searches 8
## BFGS updates skipped 0
## active bounds at final generalized Cauchy point 0
## norm of the final projected gradient 0.000234318
## final function value -2.78246
## 
## F = -2.78246
## final  value -2.782457 
## converged
</code></pre>
<pre><code class="language-r">gp.p = fn.gp(training.data.x, training.data.y,vir.total.data.x)
</code></pre>
<pre><code>## 
## optimisation start
## ------------------
## * estimation method   : MLE 
## * optimisation method : BFGS 
## * analytical gradient : used
## * trend model : ~1
## * covariance model : 
##   - type :  matern5_2 
##   - noise variances :
## [1] 1e-05 1e-05 1e-05 1e-05
##   - parameters lower bounds :  1e-10 
##   - parameters upper bounds :  1.005025 
##   - variance bounds :  0.002404243 0.3060814 
##   - best initial criterion value(s) :  2.460662 
## 
## N = 2, M = 5 machine precision = 2.22045e-16
## At X0, 0 variables are exactly at the bounds
## At iterate     0  f=      -2.4607  |proj g|=      0.96275
## At iterate     1  f =      -2.6674  |proj g|=       0.91789
## At iterate     2  f =      -2.7745  |proj g|=      0.097813
## At iterate     3  f =      -2.7792  |proj g|=       0.28285
## At iterate     4  f =      -2.7822  |proj g|=      0.093416
## At iterate     5  f =      -2.7825  |proj g|=       0.02129
## At iterate     6  f =      -2.7825  |proj g|=     0.0025827
## At iterate     7  f =      -2.7825  |proj g|=    0.00023432
## 
## iterations 7
## function evaluations 11
## segments explored during Cauchy searches 8
## BFGS updates skipped 0
## active bounds at final generalized Cauchy point 0
## norm of the final projected gradient 0.000234318
## final function value -2.78246
## 
## F = -2.78246
## final  value -2.782457 
## converged
</code></pre>
<pre><code class="language-r">mean=as.data.frame(gp.p[,1])
mean=unlist(mean)
t=total.data.x
t=unlist(t)
yy0=cbind(total.data.x,as.data.frame(mean),as.data.frame(gp.p[,1]-gp.p[,2]),as.data.frame(gp.p[,1]+gp.p[,2]))
colnames(yy0)=c(&quot;x&quot;,&quot;mean&quot;,&quot;lower&quot;,&quot;upper&quot;)
write.csv(yy0,&quot;yy0.csv&quot;)
head(yy0)
</code></pre>
<pre><code>##                      x      mean     lower     upper
## gp.p[, 1]1 0.000000000 0.3238269 0.1700662 0.4775876
## gp.p[, 1]2 0.005025126 0.3238050 0.1700444 0.4775656
## gp.p[, 1]3 0.010050251 0.3237808 0.1700203 0.4775414
## gp.p[, 1]4 0.015075377 0.3237542 0.1699937 0.4775147
## gp.p[, 1]5 0.020100503 0.3237248 0.1699643 0.4774852
## gp.p[, 1]6 0.025125628 0.3236923 0.1699320 0.4774527
</code></pre>
<pre><code class="language-r">##########data frame
data.OC.SD=matrix(,length(iter)-1,length(selector))
data.dis=matrix(,length(iter)-1,length(selector))
EI=as.data.frame(EI)
SD=matrix(0,length(iter),1)
dis=matrix(0,length(iter),1)

########utility value
EI= fn.utility(data.training,estimator,training.data,gp.p)
EI[split,]=0
EI=na.omit(EI)
ee0=cbind(total.data.x,EI)
write.csv(ee0,&quot;ee0.csv&quot;)

SD[1,1]= fn.OC(data.training,training.data)
dis[1,1]= fn.OC(data.training,training.data)

######recommend the option
num= which(EI[,&quot;EI&quot;]==max(EI[,&quot;EI&quot;]))

u=1
pic0=rep(1:10)
pic0=as.character(pic0)
pic1=rep(&quot;csv&quot;,10)
pic=paste(pic0,pic1,sep=&quot;.&quot;)
er0=rep(1:10)
er0=as.character(er0)
er1=rep(&quot;csv&quot;,10)
er=paste(er0,er1,sep=&quot;ee.&quot;)
train0=rep(1:10)
train0=as.character(train0)
train1=rep(&quot;csv&quot;,10)
train=paste(train0,train1,sep=&quot;train.&quot;)

num.count=c(split,num)
training.data0=training.data
virtual.data0=virtual.data

####start iteration
u=1
repeat{
  ########updated training data and virtual data
  training.data0[nrow(training.data0)+1,]=virtual.data0[which(virtual.data0[,&quot;order.num&quot;]==num),]
  virtual.data0=virtual.data0[-which(virtual.data0[,&quot;order.num&quot;]==num),]
  rownames(training.data0)&lt;-1:nrow(training.data0)
  rownames(virtual.data0)&lt;-1:nrow(virtual.data0)
  
  training.data0.x &lt;- data.frame(training.data0[,&quot;x&quot;])
  colnames(training.data0.x)=&quot;x&quot;
  training.data0.y &lt;-  data.frame(training.data0[,&quot;pro.es&quot;]) 
  colnames(training.data0.y)=&quot;pro.es&quot;
  virtual.data0.x=as.data.frame(virtual.data0[,&quot;x&quot;])
  colnames(virtual.data0.x)=&quot;x&quot;
  
  ##########km model prediction
  gp0 = fn.gp(training.data0.x, training.data0.y,virtual.data0.x)
  gp0.p = fn.gp(training.data0.x, training.data0.y,vir.total.data.x)
  
  ########utility value
  EI=fn.utility(data.training,estimator,training.data0,gp0.p)
  EI=as.data.frame(EI)
  EI[num.count,]=0
  EI=na.omit(EI)
  
  ######recommend the option
  num= which(EI[,&quot;EI&quot;]==max(EI[,&quot;EI&quot;]))

  mean=as.data.frame(gp0.p[,1])
  mean=unlist(mean)
  t=total.data.x
  t=unlist(t)
  
  SD[u+1,]=fn.OC(data.training,training.data0)
  dis[u+1,]=fn.dis(data.training, training.data0[nrow(training.data0),&quot;es&quot;])
  
  if(max(EI)==0)
  {break}
  
  if (u==1|u==2|u==3|u==4|u==5|u==6)
  { 
    yy0=cbind(total.data.x,as.data.frame(mean),as.data.frame(gp0.p[,1]-gp0.p[,2]),as.data.frame(gp0.p[,1]+gp0.p[,2]))
    colnames(yy0)=c(&quot;x&quot;,&quot;mean&quot;,&quot;lower&quot;,&quot;upper&quot;)
    write.csv(yy0,pic[u])
    ee0=cbind(total.data.x,EI)
    write.csv(ee0,er[u])
    write.csv(training.data0,train[u])
    print(&quot;training.data:&quot;)
    print(training.data0)
  }
  num.count=c(num.count,num)
  #######stopping criteria
  if(u&gt;=length(iter)-1)
    break
  
  #######points in the the tolerance of t
  if(min(abs(training.data0[,&quot;es&quot;]-data.training[tar,&quot;es&quot;]))&lt;=0.01*(max(data.training[,&quot;es&quot;])-min(data.training[,&quot;es&quot;])))
  {
    repeat{ 
      u=u+1
      yy0=cbind(total.data.x,as.data.frame(mean),as.data.frame(gp0.p[,1]-gp0.p[,2]),as.data.frame(gp0.p[,1]+gp0.p[,2]))
      colnames(yy0)=c(&quot;x&quot;,&quot;mean&quot;,&quot;lower&quot;,&quot;upper&quot;)
      write.csv(yy0,pic[u])
      ee0=cbind(total.data.x,EI)
      write.csv(ee0,er[u])
      training.data0=rbind(training.data0,training.data0[nrow(training.data0),])
      write.csv(training.data0,train[u])
      if(u&gt;6)
        break
    }
  }
  u=u+1
  
}
</code></pre>
<pre><code>## 
## optimisation start
## ------------------
## * estimation method   : MLE 
## * optimisation method : BFGS 
## * analytical gradient : used
## * trend model : ~1
## * covariance model : 
##   - type :  matern5_2 
##   - noise variances :
## [1] 1e-05 1e-05 1e-05 1e-05 1e-05
##   - parameters lower bounds :  1e-10 
##   - parameters upper bounds :  1.20603 
##   - variance bounds :  0.002399856 0.2546926 
##   - best initial criterion value(s) :  3.450167 
## 
## N = 2, M = 5 machine precision = 2.22045e-16
## At X0, 0 variables are exactly at the bounds
## At iterate     0  f=      -3.4502  |proj g|=       1.1553
## At iterate     1  f =      -3.4804  |proj g|=        1.1508
## At iterate     2  f =      -3.4885  |proj g|=        1.1483
## At iterate     3  f =      -3.5073  |proj g|=       0.35676
## At iterate     4  f =      -3.5076  |proj g|=       0.23578
## At iterate     5  f =      -3.5076  |proj g|=      0.012927
## At iterate     6  f =      -3.5076  |proj g|=    0.00018997
## At iterate     7  f =      -3.5076  |proj g|=    2.1147e-06
## 
## iterations 7
## function evaluations 10
## segments explored during Cauchy searches 8
## BFGS updates skipped 0
## active bounds at final generalized Cauchy point 0
## norm of the final projected gradient 2.11468e-06
## final function value -3.50763
## 
## F = -3.50763
## final  value -3.507627 
## converged
## 
## optimisation start
## ------------------
## * estimation method   : MLE 
## * optimisation method : BFGS 
## * analytical gradient : used
## * trend model : ~1
## * covariance model : 
##   - type :  matern5_2 
##   - noise variances :
## [1] 1e-05 1e-05 1e-05 1e-05 1e-05
##   - parameters lower bounds :  1e-10 
##   - parameters upper bounds :  1.20603 
##   - variance bounds :  0.002399856 0.2546926 
##   - best initial criterion value(s) :  3.450167 
## 
## N = 2, M = 5 machine precision = 2.22045e-16
## At X0, 0 variables are exactly at the bounds
## At iterate     0  f=      -3.4502  |proj g|=       1.1553
## At iterate     1  f =      -3.4804  |proj g|=        1.1508
## At iterate     2  f =      -3.4885  |proj g|=        1.1483
## At iterate     3  f =      -3.5073  |proj g|=       0.35676
## At iterate     4  f =      -3.5076  |proj g|=       0.23578
## At iterate     5  f =      -3.5076  |proj g|=      0.012927
## At iterate     6  f =      -3.5076  |proj g|=    0.00018997
## At iterate     7  f =      -3.5076  |proj g|=    2.1147e-06
## 
## iterations 7
## function evaluations 10
## segments explored during Cauchy searches 8
## BFGS updates skipped 0
## active bounds at final generalized Cauchy point 0
## norm of the final projected gradient 2.11468e-06
## final function value -3.50763
## 
## F = -3.50763
## final  value -3.507627 
## converged
## [1] &quot;training.data:&quot;
##           x          es order.num    pro.es
## 1 0.8944724  0.09169931       179 0.2014803
## 2 0.5577889  0.45274473       112 0.5625258
## 3 0.9246231  0.08285132       185 0.1926323
## 4 0.4221106  0.14832564        85 0.2581067
## 5 0.3216080 -0.52562021        65 0.4158392
## 
## optimisation start
## ------------------
## * estimation method   : MLE 
## * optimisation method : BFGS 
## * analytical gradient : used
## * trend model : ~1
## * covariance model : 
##   - type :  matern5_2 
##   - noise variances :
## [1] 1e-05 1e-05 1e-05 1e-05 1e-05 1e-05
##   - parameters lower bounds :  1e-10 
##   - parameters upper bounds :  1.346734 
##   - variance bounds :  0.002134012 0.2350295 
##   - best initial criterion value(s) :  4.323634 
## 
## N = 2, M = 5 machine precision = 2.22045e-16
## At X0, 0 variables are exactly at the bounds
## At iterate     0  f=      -4.3236  |proj g|=       1.2901
## At iterate     1  f =      -4.5085  |proj g|=        1.2758
## At iterate     2  f =      -4.5467  |proj g|=        1.2682
## At iterate     3  f =      -4.5781  |proj g|=       0.66127
## At iterate     4  f =      -4.5812  |proj g|=       0.21478
## At iterate     5  f =      -4.5815  |proj g|=       0.15995
## At iterate     6  f =      -4.5815  |proj g|=     0.0056834
## At iterate     7  f =      -4.5815  |proj g|=    2.4746e-05
## 
## iterations 7
## function evaluations 10
## segments explored during Cauchy searches 8
## BFGS updates skipped 0
## active bounds at final generalized Cauchy point 0
## norm of the final projected gradient 2.47462e-05
## final function value -4.58145
## 
## F = -4.58145
## final  value -4.581453 
## converged
## 
## optimisation start
## ------------------
## * estimation method   : MLE 
## * optimisation method : BFGS 
## * analytical gradient : used
## * trend model : ~1
## * covariance model : 
##   - type :  matern5_2 
##   - noise variances :
## [1] 1e-05 1e-05 1e-05 1e-05 1e-05 1e-05
##   - parameters lower bounds :  1e-10 
##   - parameters upper bounds :  1.346734 
##   - variance bounds :  0.002134012 0.2350295 
##   - best initial criterion value(s) :  4.323634 
## 
## N = 2, M = 5 machine precision = 2.22045e-16
## At X0, 0 variables are exactly at the bounds
## At iterate     0  f=      -4.3236  |proj g|=       1.2901
## At iterate     1  f =      -4.5085  |proj g|=        1.2758
## At iterate     2  f =      -4.5467  |proj g|=        1.2682
## At iterate     3  f =      -4.5781  |proj g|=       0.66127
## At iterate     4  f =      -4.5812  |proj g|=       0.21478
## At iterate     5  f =      -4.5815  |proj g|=       0.15995
## At iterate     6  f =      -4.5815  |proj g|=     0.0056834
## At iterate     7  f =      -4.5815  |proj g|=    2.4746e-05
## 
## iterations 7
## function evaluations 10
## segments explored during Cauchy searches 8
## BFGS updates skipped 0
## active bounds at final generalized Cauchy point 0
## norm of the final projected gradient 2.47462e-05
## final function value -4.58145
## 
## F = -4.58145
## final  value -4.581453 
## converged
## [1] &quot;training.data:&quot;
##           x          es order.num    pro.es
## 1 0.8944724  0.09169931       179 0.2014803
## 2 0.5577889  0.45274473       112 0.5625258
## 3 0.9246231  0.08285132       185 0.1926323
## 4 0.4221106  0.14832564        85 0.2581067
## 5 0.3216080 -0.52562021        65 0.4158392
## 6 0.9949749  0.07930558       199 0.1890866
## 
## optimisation start
## ------------------
## * estimation method   : MLE 
## * optimisation method : BFGS 
## * analytical gradient : used
## * trend model : ~1
## * covariance model : 
##   - type :  matern5_2 
##   - noise variances :
## [1] 1e-05 1e-05 1e-05 1e-05 1e-05 1e-05 1e-05
##   - parameters lower bounds :  1e-10 
##   - parameters upper bounds :  1.346734 
##   - variance bounds :  0.001997302 0.2313549 
##   - best initial criterion value(s) :  5.476989 
## 
## N = 2, M = 5 machine precision = 2.22045e-16
## At X0, 0 variables are exactly at the bounds
## At iterate     0  f=       -5.477  |proj g|=       1.2901
## At iterate     1  f =      -5.6197  |proj g|=        1.2833
## At iterate     2  f =      -5.7565  |proj g|=        1.2336
## At iterate     3  f =      -5.8676  |proj g|=       0.11814
## At iterate     4  f =      -5.9129  |proj g|=       0.21294
## At iterate     5  f =       -5.938  |proj g|=       0.74582
## At iterate     6  f =      -5.9406  |proj g|=       0.32468
## At iterate     7  f =      -5.9407  |proj g|=       0.12315
## At iterate     8  f =      -5.9407  |proj g|=    0.00082918
## At iterate     9  f =      -5.9407  |proj g|=    0.00017903
## 
## iterations 9
## function evaluations 14
## segments explored during Cauchy searches 10
## BFGS updates skipped 0
## active bounds at final generalized Cauchy point 0
## norm of the final projected gradient 0.000179035
## final function value -5.94071
## 
## F = -5.94071
## final  value -5.940712 
## converged
## 
## optimisation start
## ------------------
## * estimation method   : MLE 
## * optimisation method : BFGS 
## * analytical gradient : used
## * trend model : ~1
## * covariance model : 
##   - type :  matern5_2 
##   - noise variances :
## [1] 1e-05 1e-05 1e-05 1e-05 1e-05 1e-05 1e-05
##   - parameters lower bounds :  1e-10 
##   - parameters upper bounds :  1.346734 
##   - variance bounds :  0.001997302 0.2313549 
##   - best initial criterion value(s) :  5.476989 
## 
## N = 2, M = 5 machine precision = 2.22045e-16
## At X0, 0 variables are exactly at the bounds
## At iterate     0  f=       -5.477  |proj g|=       1.2901
## At iterate     1  f =      -5.6197  |proj g|=        1.2833
## At iterate     2  f =      -5.7565  |proj g|=        1.2336
## At iterate     3  f =      -5.8676  |proj g|=       0.11814
## At iterate     4  f =      -5.9129  |proj g|=       0.21294
## At iterate     5  f =       -5.938  |proj g|=       0.74582
## At iterate     6  f =      -5.9406  |proj g|=       0.32468
## At iterate     7  f =      -5.9407  |proj g|=       0.12315
## At iterate     8  f =      -5.9407  |proj g|=    0.00082918
## At iterate     9  f =      -5.9407  |proj g|=    0.00017903
## 
## iterations 9
## function evaluations 14
## segments explored during Cauchy searches 10
## BFGS updates skipped 0
## active bounds at final generalized Cauchy point 0
## norm of the final projected gradient 0.000179035
## final function value -5.94071
## 
## F = -5.94071
## final  value -5.940712 
## converged
## [1] &quot;training.data:&quot;
##           x          es order.num    pro.es
## 1 0.8944724  0.09169931       179 0.2014803
## 2 0.5577889  0.45274473       112 0.5625258
## 3 0.9246231  0.08285132       185 0.1926323
## 4 0.4221106  0.14832564        85 0.2581067
## 5 0.3216080 -0.52562021        65 0.4158392
## 6 0.9949749  0.07930558       199 0.1890866
## 7 0.8140704  0.14132260       163 0.2511036
## 
## optimisation start
## ------------------
## * estimation method   : MLE 
## * optimisation method : BFGS 
## * analytical gradient : used
## * trend model : ~1
## * covariance model : 
##   - type :  matern5_2 
##   - noise variances :
## [1] 1e-05 1e-05 1e-05 1e-05 1e-05 1e-05 1e-05 1e-05
##   - parameters lower bounds :  1e-10 
##   - parameters upper bounds :  1.346734 
##   - variance bounds :  0.001857471 0.2822268 
##   - best initial criterion value(s) :  7.600873 
## 
## N = 2, M = 5 machine precision = 2.22045e-16
## At X0, 0 variables are exactly at the bounds
## At iterate     0  f=      -7.6009  |proj g|=       1.2901
## At iterate     1  f =      -7.9481  |proj g|=        1.2827
## At iterate     2  f =      -8.4892  |proj g|=        1.2667
## At iterate     3  f =      -9.0555  |proj g|=        1.2369
## At iterate     4  f =      -9.0972  |proj g|=       0.43966
## At iterate     5  f =      -9.1219  |proj g|=       0.26032
## At iterate     6  f =      -9.1253  |proj g|=       0.25925
## At iterate     7  f =      -9.1256  |proj g|=       0.25531
## At iterate     8  f =      -9.1257  |proj g|=      0.020822
## At iterate     9  f =      -9.1257  |proj g|=      0.019152
## At iterate    10  f =      -9.1257  |proj g|=    1.2626e-06
## At iterate    11  f =      -9.1257  |proj g|=    1.2545e-06
## 
## iterations 11
## function evaluations 17
## segments explored during Cauchy searches 13
## BFGS updates skipped 0
## active bounds at final generalized Cauchy point 0
## norm of the final projected gradient 1.25455e-06
## final function value -9.12566
## 
## F = -9.12566
## final  value -9.125657 
## converged
## 
## optimisation start
## ------------------
## * estimation method   : MLE 
## * optimisation method : BFGS 
## * analytical gradient : used
## * trend model : ~1
## * covariance model : 
##   - type :  matern5_2 
##   - noise variances :
## [1] 1e-05 1e-05 1e-05 1e-05 1e-05 1e-05 1e-05 1e-05
##   - parameters lower bounds :  1e-10 
##   - parameters upper bounds :  1.346734 
##   - variance bounds :  0.001857471 0.2822268 
##   - best initial criterion value(s) :  7.600873 
## 
## N = 2, M = 5 machine precision = 2.22045e-16
## At X0, 0 variables are exactly at the bounds
## At iterate     0  f=      -7.6009  |proj g|=       1.2901
## At iterate     1  f =      -7.9481  |proj g|=        1.2827
## At iterate     2  f =      -8.4892  |proj g|=        1.2667
## At iterate     3  f =      -9.0555  |proj g|=        1.2369
## At iterate     4  f =      -9.0972  |proj g|=       0.43966
## At iterate     5  f =      -9.1219  |proj g|=       0.26032
## At iterate     6  f =      -9.1253  |proj g|=       0.25925
## At iterate     7  f =      -9.1256  |proj g|=       0.25531
## At iterate     8  f =      -9.1257  |proj g|=      0.020822
## At iterate     9  f =      -9.1257  |proj g|=      0.019152
## At iterate    10  f =      -9.1257  |proj g|=    1.2626e-06
## At iterate    11  f =      -9.1257  |proj g|=    1.2545e-06
## 
## iterations 11
## function evaluations 17
## segments explored during Cauchy searches 13
## BFGS updates skipped 0
## active bounds at final generalized Cauchy point 0
## norm of the final projected gradient 1.25455e-06
## final function value -9.12566
## 
## F = -9.12566
## final  value -9.125657 
## converged
## [1] &quot;training.data:&quot;
##           x          es order.num    pro.es
## 1 0.8944724  0.09169931       179 0.2014803
## 2 0.5577889  0.45274473       112 0.5625258
## 3 0.9246231  0.08285132       185 0.1926323
## 4 0.4221106  0.14832564        85 0.2581067
## 5 0.3216080 -0.52562021        65 0.4158392
## 6 0.9949749  0.07930558       199 0.1890866
## 7 0.8140704  0.14132260       163 0.2511036
## 8 0.9698492  0.07810269       194 0.1878837
## 
## optimisation start
## ------------------
## * estimation method   : MLE 
## * optimisation method : BFGS 
## * analytical gradient : used
## * trend model : ~1
## * covariance model : 
##   - type :  matern5_2 
##   - noise variances :
## [1] 1e-05 1e-05 1e-05 1e-05 1e-05 1e-05 1e-05 1e-05 1e-05
##   - parameters lower bounds :  1e-10 
##   - parameters upper bounds :  1.346734 
##   - variance bounds :  0.001724136 0.301194 
##   - best initial criterion value(s) :  11.89587 
## 
## N = 2, M = 5 machine precision = 2.22045e-16
## At X0, 0 variables are exactly at the bounds
## At iterate     0  f=      -11.896  |proj g|=      0.28877
## At iterate     1  f =      -13.181  |proj g|=        1.2201
## At iterate     2  f =      -13.318  |proj g|=         1.218
## At iterate     3  f =      -13.555  |proj g|=        1.2144
## At iterate     4  f =      -13.563  |proj g|=       0.13264
## At iterate     5  f =      -13.564  |proj g|=       0.13247
## At iterate     6  f =      -13.564  |proj g|=       0.27631
## At iterate     7  f =      -13.565  |proj g|=       0.27691
## At iterate     8  f =      -13.566  |proj g|=       0.27711
## At iterate     9  f =      -13.566  |proj g|=       0.06157
## At iterate    10  f =      -13.566  |proj g|=     0.0025077
## At iterate    11  f =      -13.566  |proj g|=    1.7474e-05
## 
## iterations 11
## function evaluations 16
## segments explored during Cauchy searches 12
## BFGS updates skipped 0
## active bounds at final generalized Cauchy point 0
## norm of the final projected gradient 1.74742e-05
## final function value -13.5657
## 
## F = -13.5657
## final  value -13.565717 
## converged
## 
## optimisation start
## ------------------
## * estimation method   : MLE 
## * optimisation method : BFGS 
## * analytical gradient : used
## * trend model : ~1
## * covariance model : 
##   - type :  matern5_2 
##   - noise variances :
## [1] 1e-05 1e-05 1e-05 1e-05 1e-05 1e-05 1e-05 1e-05 1e-05
##   - parameters lower bounds :  1e-10 
##   - parameters upper bounds :  1.346734 
##   - variance bounds :  0.001724136 0.301194 
##   - best initial criterion value(s) :  11.89587 
## 
## N = 2, M = 5 machine precision = 2.22045e-16
## At X0, 0 variables are exactly at the bounds
## At iterate     0  f=      -11.896  |proj g|=      0.28877
## At iterate     1  f =      -13.181  |proj g|=        1.2201
## At iterate     2  f =      -13.318  |proj g|=         1.218
## At iterate     3  f =      -13.555  |proj g|=        1.2144
## At iterate     4  f =      -13.563  |proj g|=       0.13264
## At iterate     5  f =      -13.564  |proj g|=       0.13247
## At iterate     6  f =      -13.564  |proj g|=       0.27631
## At iterate     7  f =      -13.565  |proj g|=       0.27691
## At iterate     8  f =      -13.566  |proj g|=       0.27711
## At iterate     9  f =      -13.566  |proj g|=       0.06157
## At iterate    10  f =      -13.566  |proj g|=     0.0025077
## At iterate    11  f =      -13.566  |proj g|=    1.7474e-05
## 
## iterations 11
## function evaluations 16
## segments explored during Cauchy searches 12
## BFGS updates skipped 0
## active bounds at final generalized Cauchy point 0
## norm of the final projected gradient 1.74742e-05
## final function value -13.5657
## 
## F = -13.5657
## final  value -13.565717 
## converged
## [1] &quot;training.data:&quot;
##           x          es order.num    pro.es
## 1 0.8944724  0.09169931       179 0.2014803
## 2 0.5577889  0.45274473       112 0.5625258
## 3 0.9246231  0.08285132       185 0.1926323
## 4 0.4221106  0.14832564        85 0.2581067
## 5 0.3216080 -0.52562021        65 0.4158392
## 6 0.9949749  0.07930558       199 0.1890866
## 7 0.8140704  0.14132260       163 0.2511036
## 8 0.9698492  0.07810269       194 0.1878837
## 9 0.9648241  0.07817101       193 0.1879520
## 
## optimisation start
## ------------------
## * estimation method   : MLE 
## * optimisation method : BFGS 
## * analytical gradient : used
## * trend model : ~1
## * covariance model : 
##   - type :  matern5_2 
##   - noise variances :
##  [1] 1e-05 1e-05 1e-05 1e-05 1e-05 1e-05 1e-05 1e-05 1e-05 1e-05
##   - parameters lower bounds :  1e-10 
##   - parameters upper bounds :  1.346734 
##   - variance bounds :  0.001602056 0.2959702 
##   - best initial criterion value(s) :  16.24082 
## 
## N = 2, M = 5 machine precision = 2.22045e-16
## At X0, 0 variables are exactly at the bounds
## At iterate     0  f=      -16.241  |proj g|=        0.284
## At iterate     1  f =      -17.596  |proj g|=        1.2195
## At iterate     2  f =      -17.746  |proj g|=        1.2173
## At iterate     3  f =      -17.985  |proj g|=        1.2129
## At iterate     4  f =      -17.987  |proj g|=       0.27165
## At iterate     5  f =      -17.989  |proj g|=         1.089
## At iterate     6  f =       -17.99  |proj g|=       0.76867
## At iterate     7  f =      -17.991  |proj g|=       0.17225
## At iterate     8  f =      -17.991  |proj g|=      0.024182
## At iterate     9  f =      -17.991  |proj g|=    0.00046791
## At iterate    10  f =      -17.991  |proj g|=    7.5772e-06
## 
## iterations 10
## function evaluations 15
## segments explored during Cauchy searches 11
## BFGS updates skipped 0
## active bounds at final generalized Cauchy point 0
## norm of the final projected gradient 7.57717e-06
## final function value -17.9909
## 
## F = -17.9909
## final  value -17.990918 
## converged
## 
## optimisation start
## ------------------
## * estimation method   : MLE 
## * optimisation method : BFGS 
## * analytical gradient : used
## * trend model : ~1
## * covariance model : 
##   - type :  matern5_2 
##   - noise variances :
##  [1] 1e-05 1e-05 1e-05 1e-05 1e-05 1e-05 1e-05 1e-05 1e-05 1e-05
##   - parameters lower bounds :  1e-10 
##   - parameters upper bounds :  1.346734 
##   - variance bounds :  0.001602056 0.2959702 
##   - best initial criterion value(s) :  16.24082 
## 
## N = 2, M = 5 machine precision = 2.22045e-16
## At X0, 0 variables are exactly at the bounds
## At iterate     0  f=      -16.241  |proj g|=        0.284
## At iterate     1  f =      -17.596  |proj g|=        1.2195
## At iterate     2  f =      -17.746  |proj g|=        1.2173
## At iterate     3  f =      -17.985  |proj g|=        1.2129
## At iterate     4  f =      -17.987  |proj g|=       0.27165
## At iterate     5  f =      -17.989  |proj g|=         1.089
## At iterate     6  f =       -17.99  |proj g|=       0.76867
## At iterate     7  f =      -17.991  |proj g|=       0.17225
## At iterate     8  f =      -17.991  |proj g|=      0.024182
## At iterate     9  f =      -17.991  |proj g|=    0.00046791
## At iterate    10  f =      -17.991  |proj g|=    7.5772e-06
## 
## iterations 10
## function evaluations 15
## segments explored during Cauchy searches 11
## BFGS updates skipped 0
## active bounds at final generalized Cauchy point 0
## norm of the final projected gradient 7.57717e-06
## final function value -17.9909
## 
## F = -17.9909
## final  value -17.990918 
## converged
## [1] &quot;training.data:&quot;
##            x          es order.num    pro.es
## 1  0.8944724  0.09169931       179 0.2014803
## 2  0.5577889  0.45274473       112 0.5625258
## 3  0.9246231  0.08285132       185 0.1926323
## 4  0.4221106  0.14832564        85 0.2581067
## 5  0.3216080 -0.52562021        65 0.4158392
## 6  0.9949749  0.07930558       199 0.1890866
## 7  0.8140704  0.14132260       163 0.2511036
## 8  0.9698492  0.07810269       194 0.1878837
## 9  0.9648241  0.07817101       193 0.1879520
## 10 0.9547739  0.07863702       191 0.1884180
## 
## optimisation start
## ------------------
## * estimation method   : MLE 
## * optimisation method : BFGS 
## * analytical gradient : used
## * trend model : ~1
## * covariance model : 
##   - type :  matern5_2 
##   - noise variances :
##  [1] 1e-05 1e-05 1e-05 1e-05 1e-05 1e-05 1e-05 1e-05 1e-05 1e-05 1e-05
##   - parameters lower bounds :  1e-10 
##   - parameters upper bounds :  1.346734 
##   - variance bounds :  0.001493487 0.2790738 
##   - best initial criterion value(s) :  20.51097 
## 
## N = 2, M = 5 machine precision = 2.22045e-16
## At X0, 0 variables are exactly at the bounds
## At iterate     0  f=      -20.511  |proj g|=      0.26784
## At iterate     1  f =      -22.162  |proj g|=        1.2195
## At iterate     2  f =      -22.298  |proj g|=        1.2175
## At iterate     3  f =      -22.521  |proj g|=        1.2125
## At iterate     4  f =      -22.523  |proj g|=        1.2115
## At iterate     5  f =      -22.525  |proj g|=        1.2111
## At iterate     6  f =      -22.533  |proj g|=       0.62508
## At iterate     7  f =      -22.533  |proj g|=      0.080059
## At iterate     8  f =      -22.533  |proj g|=      0.018497
## At iterate     9  f =      -22.533  |proj g|=     0.0014853
## 
## iterations 9
## function evaluations 13
## segments explored during Cauchy searches 10
## BFGS updates skipped 0
## active bounds at final generalized Cauchy point 0
## norm of the final projected gradient 0.00148525
## final function value -22.5333
## 
## F = -22.5333
## final  value -22.533315 
## converged
## 
## optimisation start
## ------------------
## * estimation method   : MLE 
## * optimisation method : BFGS 
## * analytical gradient : used
## * trend model : ~1
## * covariance model : 
##   - type :  matern5_2 
##   - noise variances :
##  [1] 1e-05 1e-05 1e-05 1e-05 1e-05 1e-05 1e-05 1e-05 1e-05 1e-05 1e-05
##   - parameters lower bounds :  1e-10 
##   - parameters upper bounds :  1.346734 
##   - variance bounds :  0.001493487 0.2790738 
##   - best initial criterion value(s) :  20.51097 
## 
## N = 2, M = 5 machine precision = 2.22045e-16
## At X0, 0 variables are exactly at the bounds
## At iterate     0  f=      -20.511  |proj g|=      0.26784
## At iterate     1  f =      -22.162  |proj g|=        1.2195
## At iterate     2  f =      -22.298  |proj g|=        1.2175
## At iterate     3  f =      -22.521  |proj g|=        1.2125
## At iterate     4  f =      -22.523  |proj g|=        1.2115
## At iterate     5  f =      -22.525  |proj g|=        1.2111
## At iterate     6  f =      -22.533  |proj g|=       0.62508
## At iterate     7  f =      -22.533  |proj g|=      0.080059
## At iterate     8  f =      -22.533  |proj g|=      0.018497
## At iterate     9  f =      -22.533  |proj g|=     0.0014853
## 
## iterations 9
## function evaluations 13
## segments explored during Cauchy searches 10
## BFGS updates skipped 0
## active bounds at final generalized Cauchy point 0
## norm of the final projected gradient 0.00148525
## final function value -22.5333
## 
## F = -22.5333
## final  value -22.533315 
## converged
</code></pre>
<pre><code class="language-r">write.csv(dis,&quot;dis_pre.csv&quot;)
write.csv(SD,&quot;OC_pre.csv&quot;)
</code></pre>
</div>
<script src="https://cdn.jsdelivr.net/npm/prismjs@1.29.0/components/prism-core.min.js" defer></script>
<script src="https://cdn.jsdelivr.net/npm/prismjs@1.29.0/plugins/autoloader/prism-autoloader.min.js" defer></script>
</body>
</html>
